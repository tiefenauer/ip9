\section{Conclusion}\label{conclusion}

This project shows that an acceptable \textit{Forced Alignment} of text with audio can be achieved for chunks of text/audio using a simple \textit{ASR} model that was trained on as little as 1.000 minutes of audio data. This model will output inferences that are sometimes recognizeable as English, but that would never qualify for speech recognition. However, the perceived quality of the alignments is still surprisingly high, despite the relatively low similarity between transcript and alignment. 

Apart from getting the \ac{STT} model to converge and output halfway distinguishable inferences, the change from local to global alignment was crucial for this success. The \textit{Needle-Wunsch} algorithm seems to be very tolerant to missing, wrong or redundant characters. Apparently, synchronization between inferences and full transcript is done on only a few character sequences. The alignments for English are not perfect, meaning that sometimes a word at the start or end of an alignment should be assigned to the previous or following alignment. Generally however, I found it astonishing how little quality from the \ac{ASR} stage is needed. This also goes for the German \ac{ASR} model, which was trained on only 80 minutes that were inflated to 1.000 minutes. This suggests that the same results can be achieved for other languages. However, since the German training-, validation- and test-set used in this project was created from a hodgepodge of very little data from \textit{ReadyLingua}, the results should not be considered to be generally valid for any German sample. For this, the German pipeline would have to be re-evaluated on a bigger test-set and/or the \ac{STT} engin should be re-trained with a representative corpus.

Finally, it may be noteworthy that although the approach chosen in this project may work for languages within the same family, it is expected that it might fail for languages like Chinese using completely different graphological (e.g. not phoneme-based) and phonetic (e.g. different intonations relating to different meaning) concepts.

\subsection{Outlook and further work}

Although the pipeline works very well with normalized audio and transcripts, this does probably not represent exactly how the pipeline will be used by \textit{ReadyLingua} in production. Depending on its use it may be required to align the partial transcript with an unnormalized full transcript (containing uppercase letters, punctuation, annotations for intermissions, images, etc.). This evaluation on unnormalized transcripts was not done because for the \textit{LibriSpeech} corpus only the normalized transcripts were available. Efforts have been made to find the text passage in the original book corresponding to the concatenated sequence of transcripts, but this was only done by normalizing the book text too.

Furthermore, it may be interesting how the pipeline behaves with transcripts containing errors, unspoken or missing text fragments as well as audio that contains distortion like noise, music or multiple speakers. For that, corresponding recordings and transcripts have to be collected first. It might also be possible to generate such samples from the existing data through augmentation.

Because an accurate alignment represent a combination of audio and its transcription, a working pipeline could also be used to generate more training data for an \ac{STT} engine, maybe even the one used in the pipeline itself. This could then be used to recursively improve the alignment quality. It would be interesting to see if this works.

All of these topics should provide enough work for a follow-up project. Finally there are some tools encountered during the project that were not tried out because there was no time. One example is the \textit{Hunspell Checker}\footnote{\url{http://hunspell.github.io}} that could be used instead of the self-implemented spell-checker\footnote{There is a Python module available at \url{https://github.com/blatinier/pyhunspell}. Dictionaries (needed by Hunspell) for various languages can be downloaded at \url{https://github.com/wooorm/dictionaries}}.