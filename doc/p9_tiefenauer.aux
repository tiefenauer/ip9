\relax 
\providecommand\hyper@newdestlabel[2]{}
\catcode `:\active 
\catcode `;\active 
\catcode `!\active 
\catcode `?\active 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\AC@reset@newl@bel
\abx@aux@refcontext{nyt/global//global/global}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\babel@aux{english}{}
\AC@undonewlabel{acro:LM}
\newlabel{acro:LM}{{}{2}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{section*.3}{}}
\acronymused{LM}
\acronymused{LM}
\acronymused{LM}
\acronymused{LM}
\AC@undonewlabel{acro:STT}
\newlabel{acro:STT}{{}{3}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{section*.4}{}}
\acronymused{STT}
\AC@undonewlabel{acro:CTC}
\newlabel{acro:CTC}{{}{3}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{section*.5}{}}
\acronymused{CTC}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{intro}{{1}{1}{Introduction}{section.1}{}}
\AC@undonewlabel{acro:FHNW}
\newlabel{acro:FHNW}{{1}{1}{Introduction}{section*.6}{}}
\acronymused{FHNW}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Scope and overall goal}{1}{subsection.1.1}}
\AC@undonewlabel{acro:RL}
\newlabel{acro:RL}{{1.1}{1}{Scope and overall goal}{section*.7}{}}
\acronymused{RL}
\AC@undonewlabel{acro:FA}
\newlabel{acro:FA}{{1.1}{1}{Scope and overall goal}{section*.8}{}}
\acronymused{FA}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Chosen approach and previous work}{1}{subsection.1.2}}
\AC@undonewlabel{acro:VAD}
\newlabel{acro:VAD}{{1.2}{1}{Chosen approach and previous work}{section*.9}{}}
\acronymused{VAD}
\AC@undonewlabel{acro:ASR}
\newlabel{acro:ASR}{{1.2}{1}{Chosen approach and previous work}{section*.10}{}}
\acronymused{ASR}
\AC@undonewlabel{acro:RNN}
\newlabel{acro:RNN}{{1.2}{1}{Chosen approach and previous work}{section*.11}{}}
\acronymused{RNN}
\AC@undonewlabel{acro:SA}
\newlabel{acro:SA}{{1.2}{1}{Chosen approach and previous work}{section*.12}{}}
\acronymused{SA}
\acronymused{VAD}
\acronymused{ASR}
\acronymused{SA}
\acronymused{ASR}
\acronymused{SA}
\acronymused{ASR}
\acronymused{ASR}
\abx@aux@cite{ctc_paper}
\abx@aux@segm{0}{0}{ctc_paper}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Previous results and problems}{2}{subsubsection.1.2.1}}
\acronymused{VAD}
\acronymused{SA}
\AC@undonewlabel{acro:SW}
\newlabel{acro:SW}{{1.2.1}{2}{Previous results and problems}{section*.13}{}}
\acronymused{SW}
\acronymused{SA}
\AC@undonewlabel{acro:LSA}
\newlabel{acro:LSA}{{1.2.1}{2}{Previous results and problems}{section*.14}{}}
\acronymused{LSA}
\acronymused{ASR}
\acronymused{RNN}
\acronymused{LSA}
\acronymused{ASR}
\AC@undonewlabel{acro:GCS}
\newlabel{acro:GCS}{{1.2.1}{2}{Previous results and problems}{section*.15}{}}
\acronymused{GCS}
\acronymused{GCS}
\acronymused{ASR}
\acronymused{GCS}
\acronymused{STT}
\acronymused{ASR}
\acronymused{CTC}
\acronymused{RNN}
\acronymused{ASR}
\AC@undonewlabel{acro:MFCC}
\newlabel{acro:MFCC}{{1.2.1}{2}{Previous results and problems}{section*.16}{}}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{RNN}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Goal of this project}{2}{subsection.1.3}}
\acronymused{VAD}
\acronymused{LSA}
\acronymused{ASR}
\acronymused{RNN}
\acronymused{RNN}
\acronymused{RNN}
\acronymused{RNN}
\acronymused{RNN}
\acronymused{FA}
\acronymused{ASR}
\acronymused{RNN}
\acronymused{LM}
\acronymused{STT}
\acronymused{LM}
\acronymused{LM}
\acronymused{STT}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Summary}{3}{subsection.1.4}}
\acronymused{ASR}
\acronymused{STT}
\abx@aux@cite{deepspeech}
\abx@aux@segm{0}{0}{deepspeech}
\abx@aux@segm{0}{0}{deepspeech}
\abx@aux@segm{0}{0}{ctc_paper}
\abx@aux@cite{mozillajourney}
\abx@aux@segm{0}{0}{mozillajourney}
\abx@aux@cite{budget}
\abx@aux@segm{0}{0}{budget}
\abx@aux@cite{wav2letter}
\abx@aux@segm{0}{0}{wav2letter}
\abx@aux@segm{0}{0}{budget}
\abx@aux@segm{0}{0}{budget}
\abx@aux@segm{0}{0}{budget}
\abx@aux@segm{0}{0}{budget}
\abx@aux@segm{0}{0}{budget}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2}Training a Neural Network for Speech Recognition}{4}{section.2}}
\newlabel{ds}{{2}{4}{Training a Neural Network for Speech Recognition}{section.2}{}}
\acronymused{STT}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}\textit  {DeepSpeech}: A reference model}{4}{subsection.2.1}}
\AC@undonewlabel{acro:NN}
\newlabel{acro:NN}{{2.1}{4}{\textit {DeepSpeech}: A reference model}{section*.17}{}}
\acronymused{NN}
\acronymused{ASR}
\AC@undonewlabel{acro:E2E}
\newlabel{acro:E2E}{{2.1}{4}{\textit {DeepSpeech}: A reference model}{section*.18}{}}
\acronymused{E2E}
\acronymused{MFCC}
\acronymused{LM}
\AC@undonewlabel{acro:WER}
\newlabel{acro:WER}{{2.1}{4}{\textit {DeepSpeech}: A reference model}{section*.19}{}}
\acronymused{WER}
\AC@undonewlabel{acro:LER}
\newlabel{acro:LER}{{2.1}{4}{\textit {DeepSpeech}: A reference model}{section*.20}{}}
\acronymused{LER}
\acronymused{WER}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Related research}{4}{subsection.2.2}}
\acronymused{STT}
\acronymused{RNN}
\AC@undonewlabel{acro:CNN}
\newlabel{acro:CNN}{{2.2}{4}{Related research}{section*.21}{}}
\acronymused{CNN}
\acronymused{LM}
\acronymused{CTC}
\acronymused{ASR}
\abx@aux@segm{0}{0}{budget}
\acronymused{CNN}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Exploiting the \textit  {DeepSpeech} model}{5}{subsection.2.3}}
\acronymused{FA}
\acronymused{ASR}
\acronymused{WER}
\acronymused{LM}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}A simpler model}{5}{subsection.2.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}Differences between the IP8- and the IP9-model}{5}{subsubsection.2.4.1}}
\AC@undonewlabel{acro:SGD}
\newlabel{acro:SGD}{{2.4.1}{5}{Differences between the IP8- and the IP9-model}{section*.22}{}}
\acronymused{SGD}
\AC@undonewlabel{acro:DS}
\newlabel{acro:DS}{{2.4.1}{5}{Differences between the IP8- and the IP9-model}{section*.23}{}}
\acronymused{DS}
\acronymused{SGD}
\acronymused{SGD}
\abx@aux@segm{0}{0}{ctc_paper}
\abx@aux@segm{0}{0}{ctc_paper}
\acronymused{MFCC}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2}Differences between the simplified and the reference model}{6}{subsubsection.2.4.2}}
\acronymused{LM}
\acronymused{LM}
\acronymused{LM}
\acronymused{LM}
\AC@undonewlabel{acro:LSTM}
\newlabel{acro:LSTM}{{2.4.2}{6}{Differences between the simplified and the reference model}{section*.24}{}}
\acronymused{LSTM}
\acronymused{LSTM}
\acronymused{LSTM}
\acronymused{MFCC}
\acronymused{LSTM}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Architecture of the simplified model. The cell type and the activation function is indicated in brackets for each layer (FC=Fully-Connected, ReLU=Rectified Linear Unit)\relax }}{7}{figure.caption.25}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{model_architecture}{{1}{7}{Architecture of the simplified model. The cell type and the activation function is indicated in brackets for each layer (FC=Fully-Connected, ReLU=Rectified Linear Unit)\relax }{figure.caption.25}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Summary}{7}{subsection.2.5}}
\abx@aux@cite{slp3}
\abx@aux@segm{0}{0}{slp3}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3}Integrating a Language Model}{8}{section.3}}
\newlabel{lm}{{3}{8}{Integrating a Language Model}{section.3}{}}
\acronymused{LM}
\acronymused{ASR}
\acronymused{LM}
\acronymused{DS}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Measuring and improving the performance of a Speech-To-Text engine}{8}{subsection.3.1}}
\acronymused{CTC}
\acronymused{STT}
\acronymused{WER}
\acronymused{LER}
\acronymused{WER}
\acronymused{LER}
\acronymused{LER}
\acronymused{WER}
\acronymused{LER}
\acronymused{LER}
\acronymused{WER}
\acronymused{LER}
\acronymused{WER}
\acronymused{WER}
\acronymused{STT}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Language Models in Speech Recognition}{8}{subsection.3.2}}
\acronymused{LM}
\acronymused{LM}
\acronymused{NN}
\acronymused{LM}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}A simple spell checker}{8}{subsection.3.3}}
\acronymused{LM}
\acronymused{LM}
\acronymused{LM}
\acronymused{CTC}
\abx@aux@segm{0}{0}{mozillajourney}
\acronymused{LM}
\acronymused{LM}
\acronymused{LM}
\acronymused{LM}
\acronymused{CTC}
\acronymused{LM}
\acronymused{LM}
\acronymused{LM}
\acronymused{LM}
\acronymused{LM}
\acronymused{LM}
\acronymused{CTC}
\acronymused{WER}
\acronymused{LM}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Reducing the vocabulary size}{9}{subsubsection.3.3.1}}
\acronymused{LM}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Example of how the spell checker works\relax }}{10}{figure.caption.26}}
\newlabel{spell-checker}{{2}{10}{Example of how the spell checker works\relax }{figure.caption.26}{}}
\acronymused{LER}
\acronymused{LER}
\acronymused{WER}
\acronymused{WER}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Example for how a Spell-Checker (SC) can help improve the quality of an inferred transcription by changing characters and words. Audio and ground truth were taken from the \textit  {ReadyLingua} corpus and the inference was made with the pre-trained \textit  {DeepSpeech} model.\relax }}{10}{table.caption.27}}
\newlabel{ler_wer_example}{{1}{10}{Example for how a Spell-Checker (SC) can help improve the quality of an inferred transcription by changing characters and words. Audio and ground truth were taken from the \textit {ReadyLingua} corpus and the inference was made with the pre-trained \textit {DeepSpeech} model.\relax }{table.caption.27}{}}
\acronymused{LM}
\acronymused{LM}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Further thoughts and considerations}{11}{subsection.3.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Summary}{11}{subsection.3.5}}
\acronymused{LM}
\acronymused{LM}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4}Plotting a learning curve}{12}{section.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Previous corpora and their problems}{12}{subsection.4.1}}
\AC@undonewlabel{acro:LS}
\newlabel{acro:LS}{{4.1}{12}{Previous corpora and their problems}{section*.28}{}}
\acronymused{LS}
\acronymused{RL}
\acronymused{LS}
\acronymused{LS}
\acronymused{LS}
\acronymused{RL}
\acronymused{LS}
\acronymused{RL}
\acronymused{RL}
\acronymused{LS}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}The \textit  {CommonVoice} Corpus}{12}{subsection.4.2}}
\AC@undonewlabel{acro:CV}
\newlabel{acro:CV}{{4.2}{12}{The \textit {CommonVoice} Corpus}{section*.29}{}}
\acronymused{CV}
\abx@aux@segm{0}{0}{ctc_paper}
\acronymused{LS}
\acronymused{CV}
\acronymused{LS}
\acronymused{RL}
\acronymused{LS}
\acronymused{RL}
\acronymused{RL}
\acronymused{CV}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Statistics about corpora that were available for training.\relax }}{13}{table.caption.30}}
\newlabel{corpora_stats}{{2}{13}{Statistics about corpora that were available for training.\relax }{table.caption.30}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Plotting the learning curve}{13}{subsection.4.3}}
\acronymused{ASR}
\acronymused{CV}
\acronymused{CTC}
\acronymused{CTC}
\acronymused{LER}
\acronymused{WER}
\acronymused{LER}
\acronymused{WER}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Decoder dimension}{13}{subsubsection.4.3.1}}
\acronymused{CTC}
\acronymused{CTC}
\acronymused{CTC}
\acronymused{CTC}
\acronymused{LM}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}\ac {LM} dimension}{14}{subsubsection.4.3.2}}
\acronymused{LM}
\acronymused{LER}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Example of a transcription whose \ac {LER} was increased when using a spell checker\relax }}{14}{table.caption.31}}
\acronymused{LER}
\newlabel{lm_bad_example}{{3}{14}{Example of a transcription whose \ac {LER} was increased when using a spell checker\relax }{table.caption.31}{}}
\acronymused{LER}
\acronymused{WER}
\acronymused{LER}
\acronymused{LER}
\acronymused{LM}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Results and interpretation}{14}{subsection.4.4}}
\acronymused{CTC}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Learning curve for the CTC-loss while training on 1/10/100/1000 minutes of transcribed audio from the \ac {CV} corpus using the $5$-gram \ac {LM} provided by the Mozilla implementation of \textit  {DeepSpeech}\relax }}{15}{figure.caption.32}}
\acronymused{CV}
\acronymused{LM}
\newlabel{lc_loss_cv}{{3}{15}{Learning curve for the CTC-loss while training on 1/10/100/1000 minutes of transcribed audio from the \ac {CV} corpus using the $5$-gram \ac {LM} provided by the Mozilla implementation of \textit {DeepSpeech}\relax }{figure.caption.32}{}}
\acronymused{LER}
\acronymused{LER}
\acronymused{LER}
\acronymused{WER}
\acronymused{LER}
\acronymused{WER}
\acronymused{LER}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Learning curve for the \ac {LER} metric while training on 1/10/100/1000 minutes of transcribed audio from the \ac {CV} corpus with and without spelling correction with a \ac {LM}. For the lines where spelling was corrected, the $5$-gram \ac {LM} provided by the Mozilla implementation of \textit  {DeepSpeech} was used.\relax }}{16}{figure.caption.33}}
\acronymused{LER}
\acronymused{CV}
\acronymused{LM}
\acronymused{LM}
\newlabel{lc_ler_cv}{{4}{16}{Learning curve for the \ac {LER} metric while training on 1/10/100/1000 minutes of transcribed audio from the \ac {CV} corpus with and without spelling correction with a \ac {LM}. For the lines where spelling was corrected, the $5$-gram \ac {LM} provided by the Mozilla implementation of \textit {DeepSpeech} was used.\relax }{figure.caption.33}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Learning curve for the \ac {WER} metric while training on 1/10/100/1000 minutes of transcribed audio from the \ac {CV} corpus with and without spelling correction with a \ac {LM}. For the lines where spelling was corrected, the $5$-gram \ac {LM} provided by the Mozilla implementation of \textit  {DeepSpeech} was used.\relax }}{16}{figure.caption.34}}
\acronymused{WER}
\acronymused{CV}
\acronymused{LM}
\acronymused{LM}
\newlabel{lc_wer_cv}{{5}{16}{Learning curve for the \ac {WER} metric while training on 1/10/100/1000 minutes of transcribed audio from the \ac {CV} corpus with and without spelling correction with a \ac {LM}. For the lines where spelling was corrected, the $5$-gram \ac {LM} provided by the Mozilla implementation of \textit {DeepSpeech} was used.\relax }{figure.caption.34}{}}
\acronymused{LER}
\abx@aux@cite{batch_size_rnn}
\abx@aux@segm{0}{0}{batch_size_rnn}
\acronymused{LER}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Example of how the quality for inferred transcripts improves with additional training data. The \ac {LER} values were calculated against the ground truth \foreignquote {french}{\textit  {i've got to go to him}}\relax }}{17}{table.caption.35}}
\acronymused{LER}
\newlabel{training_progress}{{4}{17}{Example of how the quality for inferred transcripts improves with additional training data. The \ac {LER} values were calculated against the ground truth \foreignquote {french}{\textit {i've got to go to him}}\relax }{table.caption.35}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Comparison of the simplified model with and without dropout regularization. The average \ac {LER} was calculated over all samples from the \ac {CV} test-set. The best value is highlighted.\relax }}{17}{table.caption.36}}
\acronymused{LER}
\acronymused{CV}
\newlabel{comparison_regularized_unregularized}{{5}{17}{Comparison of the simplified model with and without dropout regularization. The average \ac {LER} was calculated over all samples from the \ac {CV} test-set. The best value is highlighted.\relax }{table.caption.36}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Regularization}{17}{subsection.4.5}}
\acronymused{LER}
\acronymused{CV}
\acronymused{LER}
\acronymused{LER}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Final thoughts and considerations}{17}{subsection.4.6}}
\acronymused{LM}
\acronymused{LM}
\acronymused{LER}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.6.1}Summary}{18}{subsubsection.4.6.1}}
\acronymused{CTC}
\acronymused{LER}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5}Measuring the performance of the pipeline}{19}{section.5}}
\newlabel{e2e}{{5}{19}{Measuring the performance of the pipeline}{section.5}{}}
\acronymused{ASR}
\acronymused{STT}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}The quality of alignments}{19}{subsection.5.1}}
\acronymused{SA}
\acronymused{SA}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Metrics to evaluate the quality of alignments\relax }}{19}{table.caption.37}}
\newlabel{alignment_quality}{{6}{19}{Metrics to evaluate the quality of alignments\relax }{table.caption.37}{}}
\acronymused{LER}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Test and results}{20}{subsection.5.2}}
\acronymused{ASR}
\acronymused{ASR}
\acronymused{LER}
\AC@undonewlabel{acro:GSA}
\newlabel{acro:GSA}{{5.2}{20}{Test and results}{section*.40}{}}
\acronymused{GSA}
\acronymused{LER}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Average values of $P$, $R$ and $F$ for a pipeline using the simplified \ac {STT} model compared to a pipeline using a state of the art model. The box represents the range where 50\% of the data points are (\ac {IQR}). The whiskers extend to the last datum less than resp. more than $1.5 \cdot IQR$. Data beyond the whiskers are outliers (marked as circles). The \textit  {DeepSpeech} model produces very accurate transcriptions and therefore very precise alignments ($P$ mean: $0.865$, median: $0.879$) and also a very high $F$-Score (mean: $0.926$, median: $0.935$). On the other hand, the simplified model produces only low-quality transcriptions, resulting in a lower Precision (mean: $0.435$, median: $0.443$). The $F$-Score is thus also lower (mean: $0.602$, median: $0.614$). Recall is very high for both pipeline variants.\relax }}{21}{figure.caption.38}}
\acronymused{STT}
\AC@undonewlabel{acro:IQR}
\newlabel{acro:IQR}{{6}{21}{Average values of $P$, $R$ and $F$ for a pipeline using the simplified \ac {STT} model compared to a pipeline using a state of the art model. The box represents the range where 50\% of the data points are (\ac {IQR}). The whiskers extend to the last datum less than resp. more than $1.5 \cdot IQR$. Data beyond the whiskers are outliers (marked as circles). The \textit {DeepSpeech} model produces very accurate transcriptions and therefore very precise alignments ($P$ mean: $0.865$, median: $0.879$) and also a very high $F$-Score (mean: $0.926$, median: $0.935$). On the other hand, the simplified model produces only low-quality transcriptions, resulting in a lower Precision (mean: $0.435$, median: $0.443$). The $F$-Score is thus also lower (mean: $0.602$, median: $0.614$). Recall is very high for both pipeline variants.\relax }{section*.39}{}}
\acronymused{IQR}
\newlabel{pipeline_boxplot_ls_en}{{6}{21}{Average values of $P$, $R$ and $F$ for a pipeline using the simplified \ac {STT} model compared to a pipeline using a state of the art model. The box represents the range where 50\% of the data points are (\ac {IQR}). The whiskers extend to the last datum less than resp. more than $1.5 \cdot IQR$. Data beyond the whiskers are outliers (marked as circles). The \textit {DeepSpeech} model produces very accurate transcriptions and therefore very precise alignments ($P$ mean: $0.865$, median: $0.879$) and also a very high $F$-Score (mean: $0.926$, median: $0.935$). On the other hand, the simplified model produces only low-quality transcriptions, resulting in a lower Precision (mean: $0.435$, median: $0.443$). The $F$-Score is thus also lower (mean: $0.602$, median: $0.614$). Recall is very high for both pipeline variants.\relax }{figure.caption.38}{}}
\acronymused{LER}
\acronymused{LER}
\acronymused{LER}
\acronymused{STT}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Summary}{21}{subsection.5.3}}
\acronymused{STT}
\acronymused{GSA}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces average \ac {LER} between transcript and alignment (left plot) and similarity between alignments produced by a pipeline using the pre-trained \textit  {DeepSpeech} model resp. the simplified model (right plot). The average \ac {LER} values when using the simplified model (mean: $0.982$, median: $0.800$) follow the same pattern like when using the reference model (mean: $0.230$, median: $0.148$), but are generally higher due to the lower quality of the transcriptions. However, this does not seem to affect the alignments produced by the two pipelines. The average similarity between alignments produced by the pipeline using the simplified model are roughly the same like when using the reference model (mean: $0.887$, median: $0.909$).\relax }}{22}{figure.caption.41}}
\acronymused{LER}
\acronymused{LER}
\newlabel{pipeline_scatterplot_ls_en}{{7}{22}{average \ac {LER} between transcript and alignment (left plot) and similarity between alignments produced by a pipeline using the pre-trained \textit {DeepSpeech} model resp. the simplified model (right plot). The average \ac {LER} values when using the simplified model (mean: $0.982$, median: $0.800$) follow the same pattern like when using the reference model (mean: $0.230$, median: $0.148$), but are generally higher due to the lower quality of the transcriptions. However, this does not seem to affect the alignments produced by the two pipelines. The average similarity between alignments produced by the pipeline using the simplified model are roughly the same like when using the reference model (mean: $0.887$, median: $0.909$).\relax }{figure.caption.41}{}}
\abx@aux@segm{0}{0}{budget}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6}Forced Alignment for other languages}{23}{section.6}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Inferring German transcripts}{23}{subsection.6.1}}
\acronymused{ASR}
\acronymused{ASR}
\acronymused{CV}
\acronymused{CV}
\acronymused{ASR}
\acronymused{RNN}
\acronymused{ASR}
\AC@undonewlabel{acro:BAS}
\newlabel{acro:BAS}{{6.1}{23}{Inferring German transcripts}{section*.42}{}}
\acronymused{BAS}
\acronymused{ASR}
\acronymused{ASR}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Data augmentation}{23}{subsection.6.2}}
\acronymused{CV}
\acronymused{CV}
\abx@aux@cite{kenlm}
\abx@aux@segm{0}{0}{kenlm}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Comparison of \ac {RL} corpus before and after data augmentation (training set only)\relax }}{24}{table.caption.43}}
\acronymused{RL}
\newlabel{corpus_synth_stats}{{7}{24}{Comparison of \ac {RL} corpus before and after data augmentation (training set only)\relax }{table.caption.43}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Creating a Language Model for German}{24}{subsection.6.3}}
\acronymused{ASR}
\acronymused{LM}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Creating a raw text corpus}{24}{subsection.6.4}}
\abx@aux@cite{nltk}
\abx@aux@segm{0}{0}{nltk}
\AC@undonewlabel{acro:OOV}
\newlabel{acro:OOV}{{6.4}{25}{Creating a raw text corpus}{section*.44}{}}
\acronymused{OOV}
\@writefile{lol}{\defcounter {refsection}{0}\relax }\@writefile{lol}{\contentsline {lstlisting}{\numberline {1}Representation in corpus}{25}{lstlisting.1}}
\acronymused{LM}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Training the \ac {LM}}{25}{subsection.6.5}}
\acronymused{LM}
\abx@aux@cite{raj_lossless}
\abx@aux@segm{0}{0}{raj_lossless}
\abx@aux@segm{0}{0}{slp3}
\abx@aux@segm{0}{0}{slp3}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.1}Data structures}{26}{subsubsection.6.5.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.2}Quantization}{26}{subsubsection.6.5.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.3}Pointer Compression}{26}{subsubsection.6.5.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.4}Building the model}{26}{subsubsection.6.5.4}}
\acronymused{LM}
\acronymused{LM}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.6}Evaluating the \ac {LM}}{26}{subsection.6.6}}
\acronymused{LM}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.1}Extrinsic and intrinsic evaluation}{26}{subsubsection.6.6.1}}
\acronymused{LM}
\acronymused{LM}
\acronymused{LM}
\acronymused{LM}
\acronymused{LM}
\abx@aux@segm{0}{0}{kenlm}
\abx@aux@cite{kenlm_estimation}
\abx@aux@segm{0}{0}{kenlm_estimation}
\abx@aux@segm{0}{0}{kenlm_estimation}
\abx@aux@segm{0}{0}{kenlm_estimation}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.2}Evaluation of KenLM}{27}{subsubsection.6.6.2}}
\acronymused{LM}
\AC@undonewlabel{acro:SRILM}
\newlabel{acro:SRILM}{{6.6.2}{27}{Evaluation of KenLM}{section*.45}{}}
\acronymused{SRILM}
\acronymused{LM}
\acronymused{LM}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.3}Evaluation of the German Wikipedia \ac {LM}}{27}{subsubsection.6.6.3}}
\acronymused{LM}
\acronymused{LM}
\acronymused{LM}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.4}Evaluation 1: Comparing scores of randomized sentences}{27}{subsubsection.6.6.4}}
\acronymused{LM}
\@writefile{lol}{\defcounter {refsection}{0}\relax }\@writefile{lol}{\contentsline {lstlisting}{\numberline {2}Representation in corpus}{27}{lstlisting.2}}
\acronymused{LM}
\acronymused{LM}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Comparison of log10-probabilities calculated for news sentences and a permutation of their words\relax }}{28}{table.caption.46}}
\newlabel{LM_evaluation}{{8}{28}{Comparison of log10-probabilities calculated for news sentences and a permutation of their words\relax }{table.caption.46}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.5}Experiment 2: Word predictor}{28}{subsubsection.6.6.5}}
\acronymused{LM}
\acronymused{LM}
\acronymused{LM}
\acronymused{LM}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Word predictions of the trained 5-gram model for continuations of the stump \foreignquote *{french}{\textit  {Ein 2007 erschienenes ...}}. The blue path represents a grammatically valid German sentence.\relax }}{29}{figure.caption.47}}
\newlabel{word_predictor}{{8}{29}{Word predictions of the trained 5-gram model for continuations of the stump \foreignquote *{french}{\textit {Ein 2007 erschienenes ...}}. The blue path represents a grammatically valid German sentence.\relax }{figure.caption.47}{}}
\acronymused{STT}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.7}\ac {STT} model performance}{29}{subsection.6.7}}
\acronymused{LER}
\acronymused{LER}
\acronymused{LS}
\acronymused{RL}
\acronymused{LS}
\acronymused{LER}
\acronymused{LER}
\acronymused{LER}
\acronymused{LER}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Impact of regularization and/or synthetisation on the progress of average \ac {LER} values. Regularization alone (left) will lead to lower \ac {LER} rates. Synthesized training data (middle) will lead to a smoother curve and lower \ac {LER} rates. Both measures combined will also combine their effects, although the trend is less obvious.\relax }}{29}{figure.caption.48}}
\acronymused{LER}
\acronymused{LER}
\acronymused{LER}
\newlabel{regularization_synthetisation}{{9}{29}{Impact of regularization and/or synthetisation on the progress of average \ac {LER} values. Regularization alone (left) will lead to lower \ac {LER} rates. Synthesized training data (middle) will lead to a smoother curve and lower \ac {LER} rates. Both measures combined will also combine their effects, although the trend is less obvious.\relax }{figure.caption.48}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.8}Pipeline performance}{30}{subsection.6.8}}
\acronymused{STT}
\acronymused{LER}
\acronymused{LER}
\acronymused{ASR}
\acronymused{ASR}
\acronymused{VAD}
\acronymused{VAD}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Average values of $P$, $R$ and $F$ for German audio/transcripts aligned with a pipeline using the simplified \ac {STT}. Precision is slightly higher than for the English pipeline (mean: , median: ) which might be attributed to the fact that the German test set was much smaller (number of samples, total audio length of all samples) and also less diverse. The F-Score is similar to the one for the English pipeline (mean: , median:)\relax }}{30}{figure.caption.49}}
\acronymused{STT}
\newlabel{pipeline_boxplot_rl_de}{{10}{30}{Average values of $P$, $R$ and $F$ for German audio/transcripts aligned with a pipeline using the simplified \ac {STT}. Precision is slightly higher than for the English pipeline (mean: , median: ) which might be attributed to the fact that the German test set was much smaller (number of samples, total audio length of all samples) and also less diverse. The F-Score is similar to the one for the English pipeline (mean: , median:)\relax }{figure.caption.49}{}}
\acronymused{ASR}
\acronymused{STT}
\acronymused{LER}
\acronymused{LER}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Average \ac {LER} (left plot) and \textit  {Levenshtein Similiarity} (right plot) between transcript and alignment. Obviously shorter samples lead to better alignments because the \ac {LER} rate of the shorter transcripts from the \textit  {ReadyLingua} corpus are much lower than the rates of the longer \textit  {PodClub} samples. Vice versa the \textit  {Levenshtein Similarity} decreases for longer samples.\relax }}{31}{figure.caption.50}}
\acronymused{LER}
\acronymused{LER}
\newlabel{pipeline_scatterplot_rl_de}{{11}{31}{Average \ac {LER} (left plot) and \textit {Levenshtein Similiarity} (right plot) between transcript and alignment. Obviously shorter samples lead to better alignments because the \ac {LER} rate of the shorter transcripts from the \textit {ReadyLingua} corpus are much lower than the rates of the longer \textit {PodClub} samples. Vice versa the \textit {Levenshtein Similarity} decreases for longer samples.\relax }{figure.caption.50}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.9}Summary}{31}{subsection.6.9}}
\acronymused{LM}
\acronymused{STT}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{32}{section.7}}
\newlabel{conclusion}{{7}{32}{Conclusion}{section.7}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Outlook and further work}{32}{subsection.7.1}}
\acronymused{CV}
\acronymused{LM}
\acronymused{LER}
\acronymused{CV}
\acronymused{LM}
\acronymused{LM}
\acronymused{WER}
\acronymused{CV}
\acronymused{LM}
\acronymused{LM}
\acronymused{STT}
\acronymused{IQR}
\acronymused{LER}
\acronymused{LER}
\acronymused{LER}
\acronymused{LER}
\acronymused{LER}
\acronymused{STT}
\acronymused{LER}
\acronymused{LER}
\acronymused{LER}
\acronymused{LER}
\acronymused{LER}
\acronymused{CV}
\acronymused{RL}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {8}Appendix}{35}{section.8}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Acronyms used in this document}{35}{subsection.8.1}}
\newlabel{acronyms}{{8.1}{35}{Acronyms used in this document}{subsection.8.1}{}}
\newacro{ASR}[\AC@hyperlink{ASR}{ASR}]{Automatic Speech Recognition}
\newacro{BAS}[\AC@hyperlink{BAS}{BAS}]{Bavarian Archive for Speech Signals}
\newacro{CNN}[\AC@hyperlink{CNN}{CNN}]{Convolutional Neural Network}
\newacro{CTC}[\AC@hyperlink{CTC}{CTC}]{Connectionist Temporal Classification}
\newacro{CV}[\AC@hyperlink{CV}{CV}]{CommonVoice}
\newacro{DS}[\AC@hyperlink{DS}{DS}]{Deep Speech}
\newacro{E2E}[\AC@hyperlink{E2E}{E2E}]{end-to-end}
\newacro{FA}[\AC@hyperlink{FA}{FA}]{Forced Alignment}
\newacro{FHNW}[\AC@hyperlink{FHNW}{FHNW}]{University of Applied Sciences}
\newacro{GCS}[\AC@hyperlink{GCS}{GCS}]{Google Cloud Speech}
\newacro{GPU}[\AC@hyperlink{GPU}{GPU}]{Graphics Processing Unit}
\newacro{GRU}[\AC@hyperlink{GRU}{GRU}]{Gated Recurrent Unit}
\newacro{GSA}[\AC@hyperlink{GSA}{GSA}]{Global Sequence Alignment}
\newacro{IQR}[\AC@hyperlink{IQR}{IQR}]{Inter-Quartile Range}
\newacro{LER}[\AC@hyperlink{LER}{LER}]{Label Error Rate}
\newacro{LM}[\AC@hyperlink{LM}{LM}]{Language Model}
\newacro{LS}[\AC@hyperlink{LS}{LS}]{LibriSpeech}
\newacro{LSTM}[\AC@hyperlink{LSTM}{LSTM}]{Long Short Term Memory}
\newacro{LSA}[\AC@hyperlink{LSA}{LSA}]{Local Sequence Alignment}
\newacro{MFCC}[\AC@hyperlink{MFCC}{MFCC}]{Mel-Frequency Cepstral Coefficients}
\newacro{NN}[\AC@hyperlink{NN}{NN}]{Neural Network}
\newacro{RL}[\AC@hyperlink{RL}{RL}]{ReadyLingua}
\newacro{RNN}[\AC@hyperlink{RNN}{RNN}]{Recurrent Neural Network}
\newacro{SA}[\AC@hyperlink{SA}{SA}]{Sequence Alignment}
\newacro{SGD}[\AC@hyperlink{SGD}{SGD}]{Stochastic Gradient Descent}
\newacro{STT}[\AC@hyperlink{STT}{STT}]{Speech-To-Text}
\newacro{OOV}[\AC@hyperlink{OOV}{OOV}]{Out Of Vocabulary}
\newacro{SRILM}[\AC@hyperlink{SRILM}{SRILM}]{the SRI Language Modelling Toolkit}
\newacro{SW}[\AC@hyperlink{SW}{SW}]{Smith Waterman}
\newacro{VAD}[\AC@hyperlink{VAD}{VAD}]{Voice Activity Detection}
\newacro{WER}[\AC@hyperlink{WER}{WER}]{Word Error Rate}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}The simple spell checker in detail}{36}{subsection.8.2}}
\newlabel{spellchecker}{{8.2}{36}{The simple spell checker in detail}{subsection.8.2}{}}
\acronymused{LM}
\acronymused{LM}
\acronymused{LM}
\acronymused{CTC}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}How \ac {CTC} works}{36}{subsection.8.3}}
\newlabel{ctc-summary}{{8.3}{36}{How \ac {CTC} works}{subsection.8.3}{}}
\acronymused{CTC}
\acronymused{CTC}
\acronymused{CTC}
\acronymused{ASR}
\acronymused{CTC}
\acronymused{CTC}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}n-Gram Language Models}{37}{subsection.8.4}}
\newlabel{n-gram-summary}{{8.4}{37}{n-Gram Language Models}{subsection.8.4}{}}
\acronymused{LM}
\acronymused{LM}
\acronymused{LM}
\acronymused{LM}
\acronymused{OOV}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.1}Perplexity, discount and smoothing}{37}{subsubsection.8.4.1}}
\acronymused{LM}
\acronymused{LM}
\acronymused{OOV}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.2}Kneser-Ney Smoothing}{37}{subsubsection.8.4.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {9}Author's declaration}{39}{section.9}}
\newlabel{LastPage}{{}{39}{}{page.39}{}}
\xdef\lastpage@lastpage{39}
\xdef\lastpage@lastpageHy{39}
