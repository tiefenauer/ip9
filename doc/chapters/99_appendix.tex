\section{Appendix}
\subsection{Acronyms used in this document}
\label{acronyms}
\begin{acronym}[Bash]
	\acro{ASR}{Automatic Speech Recognition}
	\acro{BAS}{Bavarian Archive for Speech Signals}
	\acro{CNN}{Convolutional Neural Network}
	\acro{CTC}{Connectionist Temporal Classification}
	\acro{CV}{CommonVoice}
	\acro{DS}{Deep Speech}
	\acro{E2E}{end-to-end}
	\acro{FA}[FA]{Forced Alignment}
	\acro{FHNW}{University of Applied Sciences}
	\acro{GCS}{Google Cloud Speech}
	\acro{GPU}[GPU]{Graphics Processing Unit}
	\acro{GRU}{Gated Recurrent Unit}
	\acro{GSA}{Global Sequence Alignment}
	\acro{IQR}{Inter-Quartile Range}
	\acro{LER}{Label Error Rate}
	\acro{LM}{Language Model}
	\acro{LS}{LibriSpeech}
	\acro{LSTM}{Long Short Term Memory}
	\acro{LSA}{Local Sequence Alignment}
	\acro{MFCC}{Mel-Frequency Cepstral Coefficients}
	\acro{NN}{Neural Network}
	\acro{RL}{ReadyLingua}
	\acro{RNN}{Recurrent Neural Network}
	\acro{SA}{Sequence Alignment}
	\acro{SGD}{Stochastic Gradient Descent}
	\acro{STT}{Speech-To-Text}
	\acro{OOV}{Out Of Vocabulary}
	\acro{SRILM}{the SRI Language Modelling Toolkit}
	\acro{SW}{Smith Waterman}
	\acro{VAD}{Voice Activity Detection}
	\acro{WER}{Word Error Rate}
\end{acronym}

\subsection{The simple spell checker in detail}
\label{spellchecker}

Let $S$ be a sentence as a sequence of words, $V$ the vocabulary of known words used by a \ac{LM}, $p_{LM}(g)$ the score (log-based probability, likelihood) calculated by the \ac{LM} for $n$-gram $g$, $ed(s_1,s_2)$ the \textit{Levenshtein Distance} between string $s_1$ and $s_2$ and $b$ the beam width use for beam search:

\begin{itemize}
	\item for each word $w_i \in S$ check the spelling by generating the set $W'$ of possible corrections by looking it up in $V$ as follows:
	\begin{itemize}
		\item if $w_i \in V$ its spelling is already correct and $w_i$ is kept as the only possible correction, i.e.
		\begin{equation*}
		W_i = W_i^0\{ w_i \}
		\end{equation*}
		\item if $w_i \not\in V$ generate $W_i^{'}$ as the set of all possible words $w_i^{'}$ with $ed(w_i, w_i^{'}) = 1$. This is the combined set of all possible words with one character inserted, deleted or replaced. Keep the words from this combined set that appear in $V$, i.e.
		\begin{equation*}
		W_i = W_i^{'} = \left \{ w_i^{'} \mid ed(w_i, w_i^{'}) = 1 \land w_i^{'} \in V \right \}
		\end{equation*}
		\item if $W_i^{'} = \emptyset$ generate $W_i^{''}$ as the set of all possible words $w_i^{''}$ with $ed(w_i, w_i^
		{''}2) = 2$. $W_i^{''}$ can be recursively calculated from $W_i^{''}1$. Again only keep the words that appear in $V$, i.e.
		\begin{equation*}
		W_i = W_i^{''} = \left \{ w_i^{''} \mid ed(w_i, w_i^{''}) = 2 \land w_i^{''} \in V) \right \}
		\end{equation*}
		\item if $W_i^{''} = \emptyset$ keep $w_i$ as the only word, accepting that it might be either misspelled, a wrong word, gibberish or simply has never been seen by the \ac{LM}, i.e.
		\begin{equation*}
		W_i = W_i{>2} = \left \{ w_i  \right \}
		\end{equation*}
	\end{itemize}
	\item for each possible continuation in $W_i$ build the set $P$ of likelihoods for all possible 2-grams with the possible spellings in the next word as the cartesian product of all words, i.e.
	\begin{flalign*}
	&	P = \{ p_{LM}(w_j, w_{j+1}) | w_j \in W_j \land w_{j+1} \in W_{j+1} \}{,} && W_j \in \{W_i^0, C_i^{''}, C_i^{''}, W_i^{>2} \}
	\end{flalign*}
	\item keep the $b$ 2-grams with the highest likelihood and continue with recursively with the next word
\end{itemize}

\subsection{How CTC works}
\label{ctc-summary}

This is only a very short summary of how \ac{CTC} works. Awni Hannun, one of the co-authors of the \textit{DeepSpeech} paper, wrote a very comprehensive explanation and put it online\footnote{\url{https://distill.pub/2017/ctc/}}.

In a nutshell, \ac{CTC} aligns the $T_y$ characters from a known transcription (\textit{label} or \textit{ground truth}) with the $T_x$ frames from the input audio signal during training. $T_x$ is typically much larger than $T_y$ and must not be shorter. The characters (\textit{tokens}) in the label must come from an alphabet of size $V$, which for English are the 26 lowercased ASCII characters $a..z$, the space character and the apostrophe (because this character is very common in contracted words like e.g. \textit{"don't"} or \textit{"isn't"}). Additionally, \ac{CTC} introduces a special token $\epsilon$, called the \textit{blank token}, which can be used to label unknown/silent frames or prevent collapsing (see below). Consequently, the number of characters in the alphabet used by the \ac{ASR} in this project to recognize English is $|V|=26+1+1+1=29$.

\ac{CTC} is \textit{alignment-free}, i.e. it does not require any prior alignment between the characters of a transcription and the frames of an audio signal. The only thing needed is the audio signal $X$ itself plus its ground truth $Y$. Each token in the ground truth can be aligned with any number of frames in the input signal. Vice versa, repeated sequences of the same characters can be collapsed, whereas the $\epsilon$ token acts as a boundary within sequences of a token to prevent collapsing into one, when there should be two (such as in \textit{f-f-o-o-$\epsilon$-o-o-o-o-d-d-d}, which should collapse to \textit{food} and not \textit{fod}). 

For each frame input signal \ac{CTC} calculates a probability distribution over the $|V|$ characters in the alphabet. This yields a $|V| \times T_x$ probability matrix for the input signal. Because $T_x \ggg T_y$, there is usually a vast amount of different valid alignments collapsing to the same ground truth. The probability of each valid alignment can now simply be calculated by traversing the probability matrix from left to right and multiplying the probabilities of each character. Because calculating the probability of each valid alignment individually would be too slow and identical prefixes between valid alignments yield identical probabilities, a dynamic programming approach is usually chosen to calculate the probabilities whereas the intermediate probability for each prefix is saved once computed.

The most probable alignment is calculated by marginalizing (i.e. summing up) over the probabilities of the individual valid alignments. This calculation yields the CTC loss as a sum of products, which is differentiable and can therefore be optimized.

\subsection{n-Gram Language Models}\label{n-gram-summary}

\ac{LM} are probabilistic models that model the likelihood of a given sequence of characters or words. The most widely used type for word-based models \ac{LM}s are $n$-gram \ac{LM}. However, such models can estimate probabilities only for words that appear in the vocabulary of the corpus they were trained on. All other words are \ac{OOV} words with a probability of $0$. The probability of a sentence can be computed using conditional probability by calculating the probabilities of each word ($1$-grams) given all its preceding words in the sentence. Getting statistically relevant high numbers for each combination of words requires huge text corpora. However, language is dynamic and new sentences can be created all the time so that no corpus would be big enough. To handle this, $n$-grams approximate the probability of a combination of words by only considering the history of the last $n$ words ($n$ denoting the order). However, above problem is still valid for $n$-grams of any order: Because of combinatorial explosion $n$-grams suffer from sparsity with increasing order. 

\subsubsection{Perplexity, discount and smoothing}

To evaluate an $n$-gram \ac{LM} a metric called \textit{perplexity} is usually used, which is the normalized inverse probability on a test set. The perplexity can be interpreted as the grade to which the \ac{LM} is "confused" by a certain $n$-gram. A high perplexity therefore corresponds to a low probability. Since the perplexity carries the probability of a certain $n$-gram in the denominator, the perplexity for \ac{OOV}-$n$-grams cannot be calculated (division by zero). To handle this efficiently, a technique called \textit{smoothing} is applied. A very rudimentary form of smoothing is \textit{Laplace Smoothing}, which assigns a minimal count of $1$ to every $n$-gram. All other counts are also increased by adding $1$. This prevents counts of zero for $n$-grams that do not appear in the training corpus. Smoothing therefore shaves off a bit of the probability mass from the known $n$-grams and moves it to the unknown $n$-grams. The factor with which the probability of a known $n$-gram is reduced is called \textit{discount}. 

\subsubsection{Kneser-Ney Smoothing}

Although with Laplace Smoothing a low probability is assigned to previously unseen $n$-grams (which results in a high perplexity), it performs poorly in application because it discounts frequent $n$-grams too much (i.e. gives too much probability to unknown $n$-grams). A better way of smoothing is achieved using \textit{Kneser-Ney Smoothing}. For unseen $n$-grams, \textit{Kneser-Ney Smoothing} estimates the probability of a particular word $w$ being the continuation of a context based on the number of contexts it appears in the training corpus. For any previously unseen $n$-gram, a word that appears in only few contexts (e.g. the word \textit{Kong}, which only follows the words \textit{King} or \textit{Hong} in most corpora) will yield a lower probability than a word that has appeared in many contexts, even if the word itself may be very frequent. The intuition behind this is that such a word is assumed less likely to be the novel continuation for any new $n$-gram than a word that has already proved to be the continuation of many $n$-grams.