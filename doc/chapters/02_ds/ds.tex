\section{Training an \ac{RNN} for \ac{ASR}}\label{ds}
As stated above, this project does not aim at training a state of the art \ac{STT} engine. Because the \ac{SW} algorithm used for local alignment is tolerant to a certain amount of errors in the transcripts, the \ac{RNN} need only be \textit{good enough} for the task at hand (\ac{FA}). If such a network can be trained under the given circumstances it could be used in the \ac{ASR} stage of the pipeline. The pipeline would then become become self-contained and would not ne dependent on a commercial solution that cannot eb tuned and whose inner workings are unknown. Furthermore such a \ac{RNN} would open up to recognizing languages for which there is not a third-party solution yet, such as Swiss German.

Hier etwas über Transfer Learning (z.B. wie in https://arxiv.org/pdf/1706.00290.pdf) und warum es nicht eingesetzt wurde (CNN anstatt RNN, Zeitaufwand). Layer Freezen bringt ausserdem offenbar auch nix. \parencite{budget}

\subsection{Exploiting the \textit{DeepSpeech} model}

A \ac{NN} that had quite an impact on \ac{ASR} was \textit{DeepSpeech} \parencite{deepspeech} which reached recognition rates near-par to human performance, despite using a comparably simpler than traditional speech systems. Because the relation between audio signal and text was learned \ac{E2E} the network was also pretty robust to distortions like background noise or speaker variation. An open source implementation of a DeepSpeech model is available from Mozilla \footnote{\url{https://github.com/mozilla/DeepSpeech}}. Since this implementation uses a \ac{LM}, the quality of the model is measured as the percentage of misspelled or wrong words (called \ac{WER}) or as the edit distance (also called Levenshtein distance or \ac{LER}). A pre-trained model for inference of English transcript can be downloaded, which achieves a \ac{WER} of just 6.5\%, which is close to what a human is able to recognize \parencite{mozillajourney}.

A model could be trained by providing training-, validation- and test-data for an arbitrary language (e.g. from the \textit{ReadyLingua} corpus). However, this is not the preferred procedure for this project for various reasons:

\begin{enumerate}
	\item The \textit{DeepSpeech} implementation was designed for \ac{ASR}. In such settings a low \ac{WER} is desirable. But this is not the main focus for this project. As a result, the architecture of the Mozilla implementation might be overly complicated for this project, although it might make sense for pure \ac{ASR} tasks. 
	\item The problem with above point is that more complex models usually require more training data. However, as for any neural network, the limiting factor for training a \ac{RNN} is often the lack of enough high quality training data. This becomes especially important when recordings in a minority language should be aligned. 
	\item The implementation requires an (optional) \ac{LM}, which is tightly integrated with the training process which might not be available for the target languages.
\end{enumerate}

For these reasons, the \ac{RNN} architecture of the \textit{DeepSpeech} model was used as a basis for a simplified version, which should (hopefully) require less training data in order to converge and still produce partial transcriptions that can be locally aligned.

\subsection{A simpler \textit{DeepSpeech} model}

An implementation of the \ac{RNN} used for \ac{STT} in the previous IP8 project was done in Python using Keras\footnote{\url{https://keras.io}}. This model is further referred to as \textit{previous model}. Unfortunately, this model did not perform well, i.e. it was not able to learn how to infer a transcript from a given sequence of feature vectors from a spectrogram.  Furthermore, performance was a big issue, although the \ac{RNN} used a simpler architecture and no computational power was needed to query a \ac{LM}. Training on aligned speech segments from the \textit{LibriSpeech} corpus was not possible within project time because it would have taken approximately two months when using a single ac{GPU}. This duration is at least consistent with the experience made by the Machine Learning team at Mozilla Research, which used a cluster of 16 \acsp{GPU} that required about a week \parencite{mozillajourney} to train a variant \footnote{the variant used \ac{MFCC} as features whereas the original paper proposed raw spectrograms} of the \ac{RNN} originally proposed in \parencite{ctc_paper}.

For this project, the previous model was closer examined to find out what works best and to help the model converge. After some changes to the previous model architecture for this project, a new model was obtained which was able to learn something meaningful, i.e. it started to infer transcripts that -- although still not perfect -- resembled the ground truth. This model is further referred to as \textit{new model}. Summarized, the following changes were made to the previous model architecture to constitute the new model:

\begin{itemize}
	\item \textbf{Optimizer}: The new model uses \ac{SGD} instead of Adam. Adam was used in the previous model because it is the Optimizer used in the Mozilla implementation of \ac{DS}. However, this optimizer did not seem to work for the simplified model.
	\item \textbf{RNN cell type}: While \ac{LSTM} cells were used for the recurrent layer in the previous model, this was changed back to a simple RNN layer without gates as they appear in \ac{GRU} or \ac{LSTM} cells in the new model. This also corresponded to the original \textit{DeepSpeech} model architecture, which did not use sophisticated gated cells in their recurrent layer for computational reasons.
	\item \textbf{number of features}: While the use of \ac{MFCC} as features was examined in the previous model, the number of features was set to $13$, a value which is found often used in acoustic modelling. The Mozilla implementation of \textit{DeepSpeech} however doubled this number to $26$, which is also what is used in the new model. In spite of the increase in the number of features, this value is still much smaller than the $160$ filter banks used in the original \textit{DeepSpeech} model. The amount of training data is therefore still expected to be smaller than in the original model.
\end{itemize}

The final model used in this project was therefore a variant of the original \textit{DeepSpeech} model with the following simplifications:

\begin{itemize}
	\item \textbf{\ac{Different application of LM}}: In the Mozilla implementation the use of a \ac{LM} is baked in with the training process, i.e. it is integrated in the decoding process. With The edit distance between prediction and ground truth is then included in the loss which is minimized. The simplified model also uses a \ac{LM}, but does not include it in the training process. Instead, the \ac{LM} is applied in some sort of post-processing to improve the quality of the decoded predictions.
	\item \textbf{Different features}: MFCC with 26 filter banks instead of Spectrogram with 161 filterbanks, because that's what the Mozilla implementation uses
	\item No convolution in first layer
	\item LSTM instead of SimpleRNN
	\item \textbf{smaller alphabet}: The Mozilla implementation uses an alphabet of 29 characters (${a,b,c,...,z, space, apostrophe, blank}$), which is also what is proposed in the \textit{DeepSpeech} paper. This is due to the fact that apostrophes are frequently found in English word tokens (like \textit{"don't"} or \textit{"isn't"}). For simplification and also because a model should be trainable for any language, this character was dropped from the alphabet used in this project and also removed from the transcripts of all training samples by the normalization process. It is expected that missing apostrophes can be fixed by post-processing the inferred transcript with a \ac{LM}.
	\item \textbf{no context}: The \textit{DeepSpech} paper proposes using combining each feature vector $x_t$ (a frame in the spectrogram) with $C \in \left\{ 5,7,9 \right\}$ context frames. This context frame was dropped to keep the nuber of features in the input layer small. As a result, the first layer in the model only depends on the $26$ features of the feature vector $x_t$.
	\item \textbf{no convolution in input layer}: The \textit{DeepSpeech} paper proposes a series of optimization to reduce computational cost. Among these optimization is a convolution over time in the input layer with by striding with step size $2$. Because the context frame was dropped in this project, the striding was also not applied in order not to lose the information from the intermediate frames.
\end{itemize}


Figure \ref{model_architecture} shows the architecture proposed in the \textit{DeepSpeech} with the changes applied for this project. It looks similar to the one shown in the paper. Note the missing context frame, the use of \ac{MFCC} features and \ac{LSTM} cells in the recurrent layer.

\begin{figure}
	\includegraphics[width=\linewidth]{./img/model_architecture.png}
	\caption{architecture of the simplified model}
	\label{model_architecture}
\end{figure}


\section{The importance of \ac{LM}s for \ac{ASR}}

Although \ac{CTC} is the cost that is optimized during training, the usual metrics to evaluate an \ac{ASR} system are \ac{WER} and \ac{LER}. The \ac{LER} is defined as the mean normalized edit distance ($ed(a, b)$ i.e. the number of insertions, deletions and changes required to produce string $b$ from string $a$) between a an inferred transcription (\textit{prediction}) and the actual transcription (\textit{ground truth} or \textit{label}). It operates on character level and is sometimes also referred to as \textit{Levensthein Distance}. The \ac{WER} builds upon the \ac{LER} but operates on word level, i.e. it represents the number of words in a inferred transcription, that need to be inserted, deleted or changed in order to arrive at the ground truth.

If a single evaluation metric is required, the \ac{WER} is often the better choice because it is more related to the way humans would assess the quality of a transcription: A transcription which might sound correct when read out loud but is full of spelling mistakes is not a good transcription. A \ac{LM} can help inferring orthographically correct words from sequences of characters detected by \ac{CTC} and hence decrease the \ac{WER}. Therefore, by using a \ac{LM} the quality of transcriptions improves perceivably, as the following example shows:

\begin{table}[!htbp]
	\centering
	\begin{tabular}{|l|l|r|}
		\hline
		\thead{transcript} & \thead{value} & \thead{\ac{LER}} \\
		\hline
		actual transcript & \code{and i put the vice president in charge of mission control} & $1.00$ \\ 
		\hline
		inference without LM & \code{ii put he bice president in charge of mission control} & $0.11$ \\ 
		\hline
		inference with LM & \code{i put the vice president in charge of mission control} & $0.08$ \\
		\hline
	\end{tabular}
	\caption{examples of infered transcripts with pre-trained DeepSpeech model with and without \ac{LM}\\(sample \code{20161203potusweeklyaddress} from the ReadyLingua corpus}
\end{table}

A \ac{LM} models probabilities of sequences of words. Therefore a \ac{LM} can be used to assign a probability to a sentence. A simple \ac{LM} that does that is the $n$-gram \ac{LM}. $n$-grams are overlapping tuples of words whose probability can be approximated by training on massive text corpora. Although a lot of research has been made in the field of using \ac{NN} for language modelling (like for machine translation), $n$-grams \ac{LM} are still widely used and often the right tool for many tasks \parencite{slp3}, because they are faster to train and require significantly less training data.

The Mozilla implementation includes an $n$-Gram \ac{LM} using \textit{KenLM}. The \ac{LM} is queried while decoding the numeric matrices produced by \ac{CTC} using \textit{Beam Search} or \textit{Best-Path} decoding. It uses a \textit{trie} and precompiled custom implementations in C of \textit{TensorFlow}-operations to maximize performance and dedicated weights for the influence the number of valid words and the \ac{LM} itself on the inferred transcription. It is therefore deeply baked in with the decoding process.

A simpler variant that is used in this project is to infer the transcriptions first with \textit{Beam Search} or \textit{Best-Path} decoding using the standard tools provided by Keras. The inferred transcriptions are then post-processed by running it through some sort of spell-checking, which is done as follows:

\begin{itemize}
	\item split the sentence into words 
	\item for each word $w_i$ in the sentence check the spelling by generating the set $C_i$ of possible corrections by looking it up in $V$, the vocabulary of the \ac{LM}, as follows:
	\begin{itemize}
		\item if $w_i \in V$ its spelling is already correct and $w_i$ is kept as the only possible correction, i.e.
		\begin{equation*}
			C_i = C_j^0 = \{ w_i \}
		\end{equation*}
		\item if $w_i \not\in V$ generate $C_i^1$ as the set of all possible words $w_i^1$ with $ed(w_i, w_i^1) = 1$. This is the combined set of all possible words with one character inserted, deleted or replaced. Keep the words from this combined set that appear in $V$, i.e.
		\begin{equation*}
			C_i = C_i^1 = \left \{ w_i^1 \mid (w_i, w_i^1) = 1 \land w_i^1 \in V \right \}
		\end{equation*}
		\item if $C_i^1 = \emptyset$ generate $C_i^2$ as the set of all possible words $w_i^2$ with $ed(w_i, w_i^2) = 2$. $C_i^2$ can be recursively calculated from $C_i^1$. Again only keep the words that appear in $V$, i.e.
		\begin{equation*}
			C_i = C_i^2 = \left \{ w_i^2 \mid ed(w_i, w_i^2) = 2 \land w_i^2 \in V) \right \}
		\end{equation*}
		\item if $C_i^2 = \emptyset$ keep $w_i$ as the only word, accepting that it might be either misspelled, a wrong word, gibberish or simply has never been seen by the \ac{LM}, i.e.
		\begin{equation*}
			C_i = C_i{>2} = \left \{ w_i  \right \}
		\end{equation*}
	\end{itemize}
	\item for each possible spelling in $C_i$ build the set $P$ of all possible 2-grams with the possible spellings in the next word as the cartesian product of all words, i.e.
	\begin{flalign*}
&	P = \{ (w_j, w_{j+1} | w_j \in C_j \land w_{j+1} \in C_{j+1} \}{,} && C_j \in \{C_i^0, C_i^1, C_i^2, C_i^{>2} \}
	\end{flalign*}
	\item score each 2-gram calculating the log-based probability using a pre-trained 2-gram-\ac{LM}
\end{itemize}

\section{Plotting a learning curve}

The following three corpora were available for training from the previous IP8 project:

\begin{itemize}
	\item \textbf{\ac{LS}}: This corpus was created in the IP8 project from raw data from OpenSLR\footnote{\url{http://www.openslr.org/12/}}. Samples were extracted by exploiting metadata from the corpus. The corpus consists of a number of audio files containing  recordings from different speakers. Each recording contained several aligned samples. A binary index file contained the mapping to the audio file as well as the temporal information for each sample. The raw data was already divided into training-, validation- and test-set, respecting a similar distribution over the sets. This corpus was not changed for this project.
	\item \textbf{\ac{RL}}: This corpus was created from raw data provided by \textit{ReadyLingua}. Since the raw data contained recordings in several languages, separate subsets were created for each language. The corpus format is equal to the \ac{LS} corpus described above.
\end{itemize}

The following corpus was considered additionally for this project:

\begin{itemize}
	\item \textbf{\ac{CV}}\footnote{\url{https://voice.mozilla.org/en/data}}: This corpus is built and maintained by the Mozilla Foundation. There are datasets for various languages. The English version of the corpus was used by Mozilla's own \textit{DeepSpeech} implementation and is also used in this project. It contains recordings of different contributors from all over the world. The datasets are publicly available and come already divided into training-, validation- and test-set. Each set consists of an individual audio file per sample and a CSV file containing the transcriptions for each sample.
\end{itemize}

Table \ref{corpora_stats} shows some statistics about the corpora described above.

\begin{table}[!htbp]
	\centering
	\begin{tabular}{|l|r|r|r|}
		\hline
		\thead{Corpus} & \thead{total audio length} & \thead{avg sample length} & \thead{\# samples} \\
		\hline
		LibriSpeech & $44 days, 16:51:03$ & $12.3202$ & $313,491$ \\ 
		ReadyLingua (English samples) & $4:18:51$ & $3.3538$ & $4,631$ \\ 
		CommonVoice & $9 days, 15:30:05$ & $4.3118$ & $193,284$ \\ 
		\hline
	\end{tabular}
	\caption{Statistics about corpora that were available for training}
	\label{corpora_stats}
\end{table}

The model in the IP8 project was supposed to be trained on the \textit{LibriSpeech} corpus, because this corpus was much larger than the \ac{RL} corpus. It became clear however that training on all samples from this corpus was not feasible within project time because training time would have taken more than two months. It also turned out in the course of this project that the \ac{LS} corpus was probably less useful than initially assumed because the average sample length was much longer than the samples in the \ac{RL} corpus. Therefore, the newly added \ac{CV} corpus was used as main corpus for training because it was both much larger than the \ac{RL} corpus (although not as large as the \ac{LS} corpus) while providing samples with similar average length at the same time.

Although the training time on all samples of the \ac{CV} corpus is still too long for the available project time, we can still get an estimate of the learning progress by plotting a \textit{learning curve}. For this, training was done on exponentially increasing amounts of training data (1, 10, 100 and 1,000 minutes of transcribed audio). Training was done observing the behavior of the \ac{CTC}-loss for training and validation. Additionally, the progress of the \ac{WER}/\ac{LER}-metrics were observed along the dimensions:

\begin{itemize}
	\item \textbf{Decoder dimension}: Since the \ac{CTC} paper proposes two different decoding strategies (\textit{best-path} and \textit{beam search}) \cite{ctc_paper}, progress of the \ac{WER} and \ac{LER} was measured separately for both strategies.
	\item \textbf{LM dimension}: Since spelling correction \ac{LM} as described above does not necessarily lead to a more accurate transcript, the \ac{WER} and \ac{LER} metrics were measured both with and without using the \ac{LM} .
\end{itemize}

Figure \ref{lc_loss_cv} shows the learning curve for the \ac{CTC}-loss\ac{WER} and \ac{LER} without using a \ac{LM}. Figure \ref{lc_metrics_cv} shows the learning curve for training with a 4-gram \ac{LM}. To assess the impact of both the decoder- and the \ac{LM} dimension, the plot contains two curves.
: 

\begin{figure}
	\includegraphics[width=\linewidth]{./img/lc_loss_cv.png}
	\caption{Learning curve for the CTC-loss while training on 1/10/100/1000 minutes of transcribed audio from the \ac{CV} corpus using the $5$-gram \ac{LM} provided by the Mozilla implementation of \textit{DeepSpeech}}
	\label{lc_loss_cv}
\end{figure}

\begin{figure}
	\includegraphics[width=\linewidth]{./img/lc_metrics_cv.png}
	\caption{Learning curve for the \ac{WER} and \ac{LER} metric while training on 1/10/100/1000 minutes of transcribed audio from the \ac{CV} corpus with and without spelling correction with a \ac{LM}. For the lines where spelling was corrected, the $5$-gram \ac{LM} provided by the Mozilla implementation of \textit{DeepSpeech} was used.}
	\label{lc_metrics_cv}
\end{figure}

%Hier etwas über
%
%- Batchgrösse (Qualität nimmt mit zunehmender batchgrösse ab!)
%- Padding der Trainingssequenzen
%- Sortierung der Trainingsdaten nach Länge (ist besser, da weniger blanks gepadded, schnellere Konversion)
%- ...
%
%Beispiel für Prediction, wo LER/WER durch LM verschlechtert wurde: (aus /home/daniel/_runs/lc_bestpath_rlen/1000_min_bestpath/test_results.csv)
%
%#: 546	
%ground truth: i want to wish you a very happy thanksgiving	
%prediction: oento wiceyouepery appy thangksive (LER=0.431818181818182, WER=1)
%prediction (LM-corrected): onto wiceyouepery app thangksive	(LER=0.454545454545455	WER=1)
%
%
%
% To do:
% - train on LibriSpeech data
% - train using Best-Path
% - train using no LM
% - Reset calculation of WER/LER to original implementation