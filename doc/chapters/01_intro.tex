\section{Introduction}\label{intro}
This report documents the progress of the project \textit{Speech-To-Text Engine for Forced Alignment}, my Master Thesis (referred to as \textit{IP9}) at \ac{FHNW}. The corresponding code is available from GitHub\footnote{\url{https://github.com/tiefenauer/ip9}}. Some of the documentation requires the understanding of well-known concepts commonly found in research areas like \textit{Speech Recognition} or \textit{Natural Language Processing}. These concepts have often already been extensively described elsewhere. So as not to impede the reading flow, a short summary is given in the appendix and referenced where necessary. This document also frequently uses technical terms. To keep the word count low, these terms are written out only upon first usage and are afterwards replaced by their acronyms. A list of acronyms is given \hyperref[acronyms]{at the end of the document} for reference.

This project builds upon preliminary work done in a previous project (referred to as \textit{IP8}). The overall goal, project situation and some background information are described in detail in the project report for IP8 and shall not be repeated here. Only a quick recap of terms and aspects is given as far as they are relevant for the understanding of this document.

\subsection{Scope and overall goal}
\ac{RL} is a Swiss-based company that develops tools and produces content for language learning. Some of this content is given in the form of audio/video data with accompanying transcripts. These transcripts need to be enriched with temporal information, so that for each part of a transcript the corresponding location in the audio/video data can be found. Up to today, this has been done manually, which is a very time consuming process. An \textit{InnoSuisse} project was started in 2018 to research how this could be automated. The automatic mapping of orthographic transcriptions to audio is called \textit{\ac{FA}}. The \textit{InnoSuisse} project plan contains three different approaches, one of which is pursued in this project.

\subsection{Chosen approach and previous work}
The approach chosen for this project is based on speech pauses, which can be detected using \textit{\ac{VAD}}. The utterances in between are transcribed using \textit{\ac{ASR}}, for which a \textit{\ac{RNN}} is used. The resulting partial transcripts contain the desired temporal information and can be matched up with the full transcript by means of \textit{\ac{SA}}.

All theses parts can be treated as stages of a pipeline:

\begin{itemize}
	\item \textbf{\ac{VAD}}: the audio is split into non-silent parts (\textit{voiced segments})
	\item \textbf{\ac{ASR}}: each voiced segment is transcribed resulting in a partial (possibly faulty) transcript
	\item \textbf{\ac{SA}}: each partial transcript is localized within the original transcript	
\end{itemize}

Since the quality of the \ac{ASR} stage has an imminent impact on the subsequent \ac{SA} stage, the quality of the alignments depends heavily on the quality of the partial transcripts. This makes the \ac{ASR} stage the crucial stage in the pipeline. However, \ac{ASR} is highly prone to external influences like background noise, properties of the speaker (gender, speaking rate, pitch, loudness). Apart from that, language is inherently abiguous (e.g. accents), inconsistent (e.g. linguistic subtleties like homonyms or homophones) and messy (stuttering, unwanted repetitions, mispronunciation).

\subsubsection{Previous results and problems}
For the \ac{VAD} stage, an implementation\footnote{\url{https://github.com/wiseman/py-webrtcvad}} of \textit{WebRTC}\footnote{\url{https://webrtc.org/}} was used. This implementation has proved capable of detecting utterances with very high accuracy within reasonable time. For the \ac{SA} stage a combination of the \textit{\ac{SW}} algorithm and the \textit{Levenshtein Distance} was used to produce a local alignment for each partial transcript. This combination included tunable parameters like the minimum required similarity between a partial transcript and its alignment. It was able to localize potentially erroneous partial transcripts within the full transcript pretty well, provided the similarity between actual and predicted text was high enough. Since each partial transcript was aligned in isolation, the \ac{SA} stage was in fact a \textit{\ac{LSA}} stage. 

For the \ac{ASR} stage on the other hand, no \ac{RNN} could be trained that was capable of transcribing the audio segments with a quality high enough for the \ac{LSA} stage. The main problems were the lack of readily available high-grade training data, a very long training time and consequently also very long feedback cycles. Because the \ac{ASR} stage is at the heart of the pipeline, the self-trained model was replaced by a proprietary solution from \textit{Google}: API-calls to \textit{\ac{GCS}}\footnote{\url{https://cloud.google.com/speech-to-text/}} provided the necessary partial transcripts. Using this engine, the pipeline was able to produce very good (although not perfect) transcripts for the individual utterances. Therefore the chosen approach was validated and the pipeline could be shown to be generally functional. On the other hand, embedding \ac{GCS} in the \ac{ASR} stage made the whole pipeline dependent on a commercial product whose inner workings remain unknown and who cannot be tuned to the project's needs. Furthermore, although the transcripts produced by \ac{GCS} are very accurate, this quality might be an overkill for the purpose of this project. Last but not least the API calls are subject to charges. When used on large amounts of data, the use of the pipeline will incur considerably high costs. For these reasons, a partial goal of this project is to research under what circumstances a standalone \textit{\ac{STT}} model can be trained which is able to infer transcripts with sufficiently high quality.

The IP8 project proposed the use of \textit{DeepSpeech} for the \ac{ASR} stage, which uses \textit{\ac{CTC}} \parencite{ctc_paper} as its cost function. Some experiments were made to find out what features can be used to train a \ac{RNN} for the \ac{ASR} stage. The features considered were raw power-spectrograms (as stipulated by the \textit{DeepSpeech} paper), Mel-Spectrograms and \ac{MFCC}. It was found that training on \textit{\ac{MFCC}} features would probably require the least amount of training data because. An \ac{RNN} using a simplified version of the \textit{DeepSpeech} architecture was trained on data from the \textit{LibriSpeech} project (containing only English samples).

\subsection{Goal of this project}

In this project, the chosen pipelined approach shall further be refined. Because the \ac{VAD} stage already works pretty well, the focus in this project lies on the \ac{ASR} and the \ac{SA} stage. Because the pipeline should become language-agnostic and self-contained, a \ac{RNN} must be trained that can be used in the \ac{ASR} stage in the pipeline. Such a \ac{RNN} could be a simplified variant of the \textit{DeepSpeech} model, like the one implemented in the IP8 projects. 

The sequence alignment stage in the pipeline is tolerant to a certain amount of errors in the transcripts. This means training the \ac{RNN} will happen under the following premises:

\begin{itemize}
	\item The \ac{RNN} should be as simple as possible and as complex as necessary.
	\item The \ac{RNN} only needs to be \textit{good enough} for the task at hand which is \ac{FA} and not Speech Recognition.	
\end{itemize}

The reason for the first premise is that more complex neural networks usually require more training data. A network architecture requiring only little training data opens up to minority languages like Swiss German, where training data might be scarce.

The reason for the second premise data efficiency. While a simpler model will probably not be able to produce transcripts with the same accuracy as a complex model, this quality may not be required for \ac{FA} in the first place. 

The goal of this project is therefore to make statements as to under what conditions the \ac{ASR} stage can be implemented. For this, various combinations of network or data properties are explored as well as varying amounts of training data. Concretely, the following questions shall be addressed:

\begin{itemize}
	\item \textbf{How does the quality of the simplified \textit{DeepSpeech}-\ac{RNN} change with increasing training data?} By plotting the learning curve we should be able to see whether the RNN is able to learn something useful at all and also get some intuition about how much training data is needed to get reasonably accurate partial transcripts.
	\item \textbf{How does the quality of the partial transcripts change when using synthesized training data?} Neural Network usually require large amounts of training data and often improve with increasing size of the training set. However, labelled training data is usually difficult and/or expensive to acquire. Working with audio data however, synthesized training data can be easily obtained by adding some distortion to the original signal (reverb, change of pitch, change of tempo, etc.).
	\item \textbf{How does the quality of the partial transcript change when integrating a \ac{LM}?} \ac{STT}-engines traditionally use a \ac{LM} that models the probabilities of characters, words or sentences. A \ac{LM} can help producing valid transcripts by mapping sequences of characters that may be phonetically similar to the audio signal to orthographically correct sentences.
	\item \textbf{How can we assess the quality of the alignments?} This should give us some insight about how the quality of the alignment changes with varying capability of the \ac{STT}-engine and what quality of transcripts is required.
\end{itemize}

Answering above questions should help estimating the effort to create a generic pipeline.\footnote{Because \ac{ASR} is highly dependent on the language that should be recognized, a different \ac{STT} system has to be trained for each language.}

\subsection{Summary}
This chapter gave an introduction to the project, its scope and goal as well as how it is embedded in the \textit{InnoSuisse} project. It also gave a quick overview over the preliminary work done in the IP8 project by outlining problems and impediments experienced there.