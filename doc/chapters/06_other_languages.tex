\section{Forced Alignment for other languages}

So far, only audio and transcripts in English were considered. A fully automated solution however should be able to align text and audio in any other language. Because of linguistic characteristics like sound patterns and morphology the results might vary a lot between languages when tested under identical circumstances. To get some intuition about the influence of language and whether above conclusions are transferable to other languages, the pipeline was evaluated on the German samples received from \textit{ReadyLingua}.

\subsection{Inferring German transcripts}

Enabling the pipeline to handle German samples means training a German \ac{ASR} as its core element. This requires minimal modifications to the network architecture, because German transcripts use a different alphabet. As mentioned before, the apostrophe is far less common in German than it is in English and was therefore dropped. On the other hand, umlauts are very common in German and were added to the 26 ASCII characters. Since the alphabet represents all possible labels, the output layer in the model needs to be changed to contain 31 units (one for each character in the alphabet, the three umlauts, space plus a blank token) instead of the 29 units used for English.

Training an \ac{ASR} model for German and plotting a learning curve also requires amounts of training data on a similar scale like the \ac{CV} corpus used for English. Since at the time of this writing, the \ac{CV} corpus was still a work in progress, datasets for languages other than English were not available. High-Quality speech corpora for \ac{ASR} are generally hard to find, especially considering the number of samples needed to train a \ac{RNN}. There are corpora for \ac{ASR} in German, but those are often liable to pay costs. An extensive list of German corpora for various purposes can be found at the \textit{\ac{BAS}}\footnote{\url{https://www.phonetik.uni-muenchen.de/Bas/BasKorporaeng.html}}, including exotic corpora like the \textit{ALC} corpus\footnote{\url{http://www.bas.uni-muenchen.de/forschung/Bas/BasALCeng.html}} containing recordings of intoxicated people. Some of the corpora on this list are free for scientific usage and have been used by \cite{budget} to train their German \ac{ASR} model with Transfer Learning. However, not all of these corpora are targeted at \ac{ASR} and their quality is often unknown.

\subsection{Data augmentation}

Integrating new raw data means preprocessing the audio (e.g. resampling) and the text (e.g. normalization, tokenization) to make sure it exhibits the same properties as the other corpora and the data conforms to the target format. This step is usually very time consuming, often taking most of the project time. Because no ASR corpus for German was readily available, training was done on the data received from \textit{ReadyLingua} as a start. The alignment between audio and transcript in this corpus was done manually and is therefore very accurate. Audio and text were already preprocessed in the IP8 project and the metadata was processed and stored as a corpus. The individual training samples could therefore be transformed to the format expected by the Mozilla implementation of \textit{DeepSpeech} (and thus by the simplified model) with comparably little effort. Also, the samples exhibited similar properties (average audio and transcript length) like the \ac{CV} corpus (see table \ref{corpora_stats}). However, the total length of the samples in the training set was only about one and a half hours, which was much less than the 1000+ minutes in the \ac{CV} corpus and certainly not enough for the $1.000$ minutes needed to plot a learning curve like to the one made for English. 

An easy way to get more training data is augmenting existing data by synthesizing new data from it. This is particularly easy for audio data, which can be distorted in order to get new samples corresponding to the same transcript. The following distortions were applied in isolation to each sample in the training set:

\begin{itemize}
	\item \textbf{Shifting}: The frames in the input signal were zero-padded with samples corresponding to a random value between $0.5$ and $1.5$ seconds, shifting the signal to the right, i.e. starting the signal later. This resulted in one additional synthesized sample for each original sample. Shifting to the left was not done to prevent cropping parts of the speech signal.
	\item \textbf{Echo}: The presence of echo can be generated with the Pysndfx library\footnote{\url{https://github.com/carlthome/python-audio-effects}} using random values for delay and damping. This resulted in one additional sample.
	\item \textbf{Pitch}: The pitch of the signal was increased or decreased. Increasing and decreasing was done using two different random factors, resulting in two additional samples. This can be seen as a rudimentary way to simulate a female from a male speaker or vice versa.
	\item \textbf{Speed}: Faster or slower speaking rates can be simulated by "stretching" or "compressing" the signal while preserving the pitch. Similar to the change in pitch, two different random factors were use to change the tempo. This resulted in two additional samples.
	\item \textbf{Volume}: The loudness of the speaker was artificially reduced or increased by a random value within the range of $[-15..-5]$ resp. $[5..15]$ db. This resulted in two additional sample.
\end{itemize}

With above methods eight synthetisized samples can be created for each original sample from the corpus. It turned out however that this was still not enough to plot a learning curve. To augment the data to the 1.000 minutes needed, additional samples were created using random combinations of the distortions. The random parameters differed from the ones used before to prevent overfitting to the distortion. Table \ref{corpus_synth_stats} shows the corpus statistics before and after data augmentation.

\begin{table}[!htbp]
	\centering
	\begin{tabular}{lrrrr}
		\toprule
		\thead{} & \thead{\textbf{total audio length}} & \thead{\textbf{\# samples}} & \thead{\textbf{Ø sample length (seconds)}} \\
		\midrule
		\textbf{before augmentation} & $1:36:09$ & $1,700$ & $2.89$ \\ 		
		\textbf{after augmentation} & $16:40:00$ & $18,955$ & $3.16$ \\ 		
		\bottomrule
	\end{tabular}
	\caption{Comparison of \ac{RL} corpus before and after data augmentation (training set only)}
	\label{corpus_synth_stats}
\end{table}


\subsection{Creating a Language Model for German}

Since the \ac{ASR} stage in the pipeline uses a spell-checker querying a \ac{LM} to post-process the results a $5$-gram model similar to the one created by Mozilla needed to be trained first. The following sections require understanding some methods of $n$-grams like smoothing, discount and backoff. A short explanation of how $n$-grams work is given \hyperref[n-gram-summary]{in the appendix}.

\subsection{Creating a raw text corpus}

To train a $n$-gram model for German, a raw text corpus of German Wikipedia articles was used as corpus. Like the English $n$-gram from Mozilla, KenLM \parencite{kenlm} was used to estimate the probabilities. The articles were pre-processed to meet the requirements of \textit{KenLM}. It was normalized as follows 

\begin{itemize}
	\item remove Wiki markup
	\item remove punctuation
	\item make everything lowercase
	\item \textbf{Unidecoding}: translate accentuated characters (like \code{è,é,ê}, etc.) and special characters (like the German \textit{ß}) to their most similar ASCII-equivalent (\code{e} resp. \code{ss}). This process helps accounting for ambiguous spelling variants of the same word and misspelled words. It also reduces the number of unique words by reducing different versions to a normalized base form. A special case are umlauts. Although also not part of the ASCII code set, they were kept as-is because they are very common in German.
	\item \textbf{Tokenization}: Because \textit{KenLM} expects the input as sentences (one sentence per line), the raw text was further tokenized into sentences and words using NLTK \parencite{nltk}. 
	\item \textbf{Numeric tokens}: Word tokens that are purely numeric (such as year numbers) are replaced with the special token \code{<num>}. Although such tokens occur frequently in the Wikipedia articles, they are unwanted in the corpus because they represent values and do not carry any semantic meaning. Because there is a infinite number of possible numeric tokens, they were all collapsed to the same normalized token.
\end{itemize}

The corpus was saved as text file containing one normalized sentence per line. The special tokens \code{<s>} and \code{</s>} are used to mark beginnings and endings of sentences as well as the \code{<unk>} token which is traditionally used to represent \ac{OOV} words. They are however not part of the corpus because they are added automatically by \textit{KenLM}.

The following lines are an excerpt of a article in the German Wikipedia along with its representation in the corpus.

\begin{displayquote}[German Wikipedia article about Speech Recognition\footnote{\url{https://de.wikipedia.org/wiki/Spracherkennung}}]
	Die Größe des Wörterbuchs hängt stark von der Sprache ab. Zum einen haben durchschnittliche deutschsprachige Sprecher mit circa 4000 Wörtern einen deutlich größeren Wortschatz als englischsprachige mit rund 800 Wörtern. Außerdem ergeben sich durch die Flexion in der deutschen Sprache in etwa zehnmal so viele Wortformen, wie in der englischen Sprache, wo nur viermal so viele Wortformen entstehen.
\end{displayquote}

\begin{lstlisting}[numbers=left, caption=Example source text from Wikipedia and its representation in the corpus used to train the 5-gram model using \textit{KenLM}]
die grösse des wörterbuchs hängt stark von der sprache ab
zum einen haben durchschnittliche deutschsprachige sprecher mit circa <num> wörtern einen deutlich grösseren wortschatz als englischsprachige mit rund <num> wörtern
ausserdem ergeben sich durch die flexion in der deutschen sprache in etwa zehnmal so viele wortformen wie in der englischen sprache wo nur viermal so viele wortformen entstehen
\end{lstlisting}

Like for the English spell checker, three vocabularies containing the $80.000$ most frequent words from the corpus was created. The words from these vocabularies make up $90.86\%$ of the total number of words in the corpus. It is expected that the optimal number of words in the vocabulary is higher for German than for English. This is due to the fact that different flexions of the same word are very common in German due to grammatical conjugations (different forms for the same verb) and declinations (different cases for the same noun). Therefore German texts tend to use a wider range of words, which might account for the fact that the top 80k words cover only about 90\% of the Wikipedia corpus (compared to $>90\%$ in the English corpus). Handling the different flexions would require lemmatization and/or stemming the corpus in order to reduce them to a common base form. This has not been done for simplicity and time constraints. It is also doubtful whether this would actually help improving the quality of inferred transcripts, since humans do not speak in lemmata or stems.

\subsection{Training the Language Model}

The final corpus contained data from 2,221,101 Wikipedia articles (42,229,452 sentences, 712,167,726 words, 8,341,157 unique words). This corpus was used to train a $5$-gram \ac{LM} using \textit{KenLM}. \textit{KenLM} uses \textit{Kneser-Ney Smoothing} and some optimization techniques called \textit{quantization} and \textit{pointer compression}. 

\subsubsection{Data structures}
$n$-grams can be represented with a prefix-tree structure (called \textit{Trie})\footnote{note that \textit{KenLM} offers a so called \textit{PROBING} data structure, which is fundamentally a hash table combined with interpolation search, a more sophisticated variant of binary search, which allows for constant space complexity and linear time complexity. This does however not change the fact that $n$-grams can conceptually be seen as a tree of grams}, which allows for pruning. $n$-grams of order 2 and higher can be pruned by setting a threshold value for each order. $n$-grams whose frequency is below the threshold will be discarded. \textit{KenLM} does not support unigram pruning.

\subsubsection{Quantization}
To save memory, the amount of bits used to store the non-negative log-probabilities can be reduced with the parameter $q$ to as little as $q = 2$ bits at the expense of accuracy. This reduction yields $2^q -1$ possible bins. The value of each bin is calculated by equally distributing the probabilities over these bins and computing the average. Note that the quantization is done separately for each order and unigram probabilities are not quantized.

\subsubsection{Pointer Compression}
To use memory even more efficiently, the pointers which are used to store $n$-grams and their probabilities can be compressed. Such pointers are used to represent e.g. word IDs (for $q$-grams) and are stored as sorted integer-arrays. Additionally, These integers can be compressed using a lossless technique from \cite{raj_lossless} by removing leading bits from the pointers and store them implicitly into a table of offsets. The parameter $a$ controls the maximum number of bits to remove. There is a time-space trade-off meaning that a higher value of $a$ will lead to a smaller memory footprint at the cost of a slower training time.

\subsubsection{Building the model}

The a $5$-gram \ac{LM} was trained on the German Wikipedia corpus using  using the Trie data structure and the same parameters like the model downloaded from \textit{DeepSpeech} ($q=8$ and $a=255$). Like the \textit{DeepSpeech} model $4$- and $5$-grams were pruned by setting a minimum frequency of $1$.

%Pruning 1-grams would help getting rid of obvious spelling mistakes and very rare tokens that only appear in very special contexts. Unfortunately, \textit{KenLM} does not support pruning unigrams, only higher-order $n$-grams. Therefore the vocabulary used for training was limited to the 500,000 most frequent words not containing numbers with a minimum length of 2 characters \footnote{this constraint was imposed because NLTK did sometimes not tokenize abbreviations like \textit{z.B.} correctly, which resulting in two separate tokens \code{z} and \code{b}}. Note that this amount is about the same what was used in the \textit{DeepSpeech} paper \parencite{deepspeech} but more than the $250.000$ words used for the English spell checker. 

%The 500k most frequent words in the vocabulary make up 96.42\% of the whole corpus. The most frequent word in the vocabulary is the \code{<num>} token (29,659,021 counts). The least frequent word is \textit{Flachmeeres} (31 counts), which is actually a derivate of another German word \textit{Flachmeer}, which also appears in the vocabulary (109 counts) and which is an example of above mentioned flexion. 

%By limiting the vocabulary to the 500k most frequent words, the unigrams in the KenLM were artificially pruned. The number of unigrams was therefore 500,003 (one unigram for each word in the vocabulary plus one each for the \code{<s>}, \code{</s>} and \code{<unk>} tokens). The $n$-grams used to train the \ac{LM} have been pruned by setting the minimal threshold for $n$-Grams of any order ($n \in 1..4$) to 40. This is the value that Google used (reference from Jurafsky). Pruning unigrams helped getting rid of obvious spelling mistakes and very rare tokens that only appear in very special contexts (like the tokens \textit{aaaaa} or \textit{zzyzzyxx}) (EDIT: Pruning unigrams is not supported by KenLM, but the vocabulary can be limited). Such words are mostly not no real German words and should therefore not be trained on. Pruning higher-order $n$-grams was done to increase performance (both in space and time).

\subsection{Evaluating the Language Model}

Literature suggests two methods to evaluate a \ac{LM}: Extrinsic and intrinsic evaluation.

\subsubsection{Extrinsic and intrinsic evaluation}

The best way to evaluate a \ac{LM}is to embed it in an application and measure how much the application improves \parencite{slp3}. This is called \textit{extrinsic evaluation} and has been done by comparing the learning curves with and without using a \ac{LM}. However, to measure the performance of a \ac{LM} independently (\textit{intrinsic evaluation}) one would have to provide a test set containing unseen sentences and assess the scores of the \ac{LM} on their $n$-grams. The results can then be compared to a reference \ac{LM}: Whatever model produces higher probabilities (or lower perplexity) to the $n$-grams in the test set is deemed to perform better. However, models can only be compared if they use the same vocabulary. Additionally, $n$-gram models always encode characteristics of the training corpus \parencite{slp3}. Since the sentences in a corpus of legal documents use different structures and word distributions than a corpus of children's books, two models trained on these corpora will not be comparable. Evaluating the created German Wikipedia corpus intrinsically would therefore require training a reference model on the same corpus, which can become very time consuming.

\subsubsection{Evaluation of KenLM}

\textit{KenLM} has been extensively compared to other \ac{LM} implementations like \ac{SRILM} both in terms of speed and accuracy. It has been found to be both faster and more memory efficient \parencite{kenlm} than the fastest alternative. Its low memory profile makes it runnable on a single machine, while other algorithms like \textit{MapReduce} target clusters \parencite{kenlm_estimation}. The highly optimized performance was a big advantage especially for this project because it enabled testing the model on a local machine. The probabilistic performance of \textit{KenLM} has been evaluated by training a $5$-gram model on a 126 billion token corpus (393 million unique words) \parencite{kenlm_estimation}. This model was embedded in some Machine Translation systems (Czech-English, French-English and Spanish-English) . Evaluation was done by calculating the BLEU score and comparing it to embeddings of other \ac{LM}. \textit{KenLM} placed first in all submissions.

\subsubsection{Evaluation of the German Wikipedia LM}

Because of time constraints and because \textit{KenLM} has already been extensively evaluated on English I resigned from evaluating my German \ac{LM} intrinsically, even though the corpus used for training is not as big as the one used in \cite{kenlm_estimation}. \textit{KenLM} is to date widely recognized as the best performing \ac{LM} available, which is also emphasized by the usage of a \textit{KenLM} model in the Mozilla implementation of \textit{DeepSpeech}.

To still get an intuition about how well the model performs (before embedding it in the \ac{ASR} stage of the pipeline), two different experiments were made:

\begin{itemize}
	\item \textbf{Experiment 1}: The probability calculated for valid German sentences was compared against variants of the same sentences with the words in randomized order.
	\item \textbf{Experiment 2}: The \ac{LM} was used together with its vocabulary to build a simple word predictor.
\end{itemize}

Both experiments are explained in more depth below.

\subsubsection{Evaluation 1: Comparing scores of randomized sentences}

The first experiment tests the validity of the probabilities (\textit{scores}) calculated by the \ac{LM}. For this, an arbitrary choice of 5 valid sentences in German was used. To ensure the sentences could not have been seen during training, the following 5 sentences were taken from a newspaper printed after the creation of the Wikipedia dump:

\begin{lstlisting}[numbers=left, caption=Representation in corpus]
Seine Pressebeauftragte ist ratlos.
Fünf Minuten später steht er im Eingang des Kulturcafés an der Zürcher Europaallee.
Den Leuten wird bewusst, dass das System des Neoliberalismus nicht länger tragfähig ist.
Doch daneben gibt es die beeindruckende Zahl von 30'000 Bienenarten, die man unter dem Begriff «Wildbienen» zusammenfasst.
Bereits 1964 plante die US-Airline Pan American touristische Weltraumflüge für das Jahr 2000.
\end{lstlisting}

All sentences have been normalized the same way sentences were preprocessed for training. For each of them the score was calculated. Then the words were shuffled and the score was calculated again. A good \ac{LM} should calculate a (much) higher probability for the original sentence, because the shuffled sentence is most likely just gibberish.  Table \ref{LM_evaluation} shows the results of the comparison. It is evident that the probabilities for the shuffled sentences are much lower than for the sentences where the words appear in the correct order. The probabilities calculated by the \ac{LM} are therefore deemed valid.

\begin{table}[!htbp]
	\centering
	\begin{tabular}{lrlr}
		\toprule
		\thead{original sentence (normalized)} & \thead{score} & \thead{permuation} & \thead{score} \\
		\midrule
		\makecell[l]{seine pressebeauftragte\\ist ratlos} & -17.58 & \makecell[l]{ist ratlos\\pressebeauftragte seine} & -21.52 \\
		\makecell[l]{fünf minuten später steht\\er im eingang des kulturcafes\\an der zürcher europaallee} & -40.23 & \makecell[l]{des er minuten zürcher kulturcafes\\steht europaallee eingang\\fünf im später an der} & -57.69 \\
		\makecell[l]{den leuten wird bewusst\\dass das system des \\ neoliberalismus nicht\\länger tragfähig ist} & -35.52 & \makecell[l]{system nicht das ist\\dass leuten tragfähig des\\neoliberalismus den\\bewusst länger wird} & -51.27 \\
		\makecell[l]{doch daneben gibt es die\\beeindruckende zahl von \code{<num>}\\bienenarten die man unter dem\\begriff wildbienen zusammenfasst} & -48.36 & \makecell[l]{dem gibt wildbienen zahl\\beeindruckende doch man\\zusammenfasst es daneben bienenarten\\von die unter die \code{<num>} begriff} & -75.95 \\
		\makecell[l]{bereits \code{<num>} plante\\die usairline pan american\\touristische weltraumflüge\\für das jahr \code{<num>}} & -58.04 & \makecell[l]{plante touristische für\\jahr pan american das\\bereits usairline \code{<num>}\\\code{<num>} weltraumflüge die} & -64.02 \\
		\bottomrule
	\end{tabular}
	\caption{Comparison of log10-probabilities calculated for news sentences and a permutation of their words}
	\label{LM_evaluation}
\end{table}

\subsubsection{Experiment 2: Word predictor}

The second experiment tests whether the trained \ac{LM} is able to continue a sentence given its beginning. For this each word from the vocabulary is appended and the score of the resulting stumps is calculated. The most likely continuation can be estimated by sorting the resulting list in descending order (the probabilities are $\log_{10}$-based, i.e. negative) and taking the top element. This behavior can be applied iteratively to construct a sentence from a stump. For this experiment a sentence was started with the stump \foreignquote*{french}{\textit{Ein 2007 erschienenes}}. Afterwards a word from the five most probable continuations was appended. The extended stump was then again fed into the \ac{LM}. This process was repeated until some kind of sentence ending was encountered. Each extended stump was preprocessed the same way the sentences were preprocessed for training (lowercasing, replacing numbers with \code{<num>}, etc.). Figure \ref{word_predictor} shows the path taken through the predictions. Note that the predictions for the second and third word of the stump after typing the first word are shown in grey for illustrative purposes, although they were not considered for continuation.

\begin{figure}[h!]
	\includegraphics[width=\linewidth]{./img/word_predictor.png}
	\caption{Word predictions of the trained 5-gram model for continuations of the stump \foreignquote*{french}{\textit{Ein 2007 erschienenes ...}}. The blue path represents a grammatically valid German sentence.}
	\label{word_predictor}
\end{figure}

Although prediction was slow we can observe that the words suggested by the \ac{LM} are generally grammatically correct continuations and often make sense, although the probability for some of the predicted words (like \textit{Michelangelo}) is sometimes unexplicably high. Nevertheless it was possible to create a valid German sentence from the stump using only the suggested words. The \ac{LM} even seems to have captured some notion about grammatical concepts like German cases (e.g. that \foreignquote{french}{\textit{die Geschichte Chinas}} is more likely than \foreignquote{french}{\textit{die Geschichte China}}). On the other hand we can observe that the meaningfulness of the suggestions decreases with the progress because some long-distance relationships between words are lost for small values of $n$.

\subsection{STT model performance}

The observations made when training the simplified Keras model on German audio data are similar to the ones made when training on English data in that the spell-checker will not help and the CTC validation loss will decrease until epoch 15 and then plateau or increase slightly.

When evaluating the \ac{LER} metric on the test set, the best performance was achieved with a regularized model that was trained on 1.000 minutes of audio, including synthesized samples. The average \ac{LER} value was then $0.4918$, which is even better than the $0.5125$ achieved when training on English samples from the \ac{LS} corpus. This result has to be taken with a pinch of salt though, because the test data in the \ac{RL} corpus is not as extensive as the one from the \ac{LS} corpus and it has also not been extracted with the same diligence. 

The effect of regularisation and/or use of synthesized training data can also be visualized. Figure \ref{regularization_synthetisation} shows both measures in isolation and in combination. From the plot on the left it becomes evident that transcripts inferred by a regularized model will generally have a lower \ac{LER} than without regularisation. The plot in the middle shows how the use of synthesized training data has a smoothing effect on the curve of the \ac{LER}. Finally, the plot on the right shows the progress of the average \ac{LER} values when combining both measures, i.e. training a regularized model with training data including synthesized samples. The effect is bot a smoother curve and mostly lower \ac{LER} rates, although there is an awkward spike between epoch 15 and 20.

\begin{figure}[h!]
	\includegraphics[width=\linewidth]{./img/regularization_synthetisation.png}
	\caption{Impact of regularization and/or synthetisation on the progress of average \ac{LER} values. Regularization alone (left) will lead to lower \ac{LER} rates. Synthesized training data (middle) will lead to a smoother curve and lower \ac{LER} rates. Both measures combined will also combine their effects, although the trend is less obvious.}
	\label{regularization_synthetisation}
\end{figure}

\subsection{Pipeline performance}

Because the regularized model trained with synthesized data has both the lowest \ac{LER} rates and a smoother curve, this model is used for evaluation of the whole pipeline. After training the model for 15 epochs, it was plugged into the pipeline to process German audio/text samples (\textit{German pipeline}). Evaluation is done like for the English pipeline by calculating $P$, $R$ and $F$ as well as the average \ac{LER} between transcript and alignment. However, because there is no reference \ac{ASR} model for German\footnote{apart from proprietary \ac{STT} engines like \textit{Google Cloud Speech}, which were deliberately left out in this project}, the alignments produced by the pipeline cannot be compared to a reference alignment. This comparison is therefore done a bit differently for the German pipeline.

Since there is segmentation information for the audio/text samples in the \textit{ReadyLingua} corpus, the audio is already split into voiced segments and aligned with text fragments (let $A_{original}$ denote this alignment information ). For lack of a German reference \ac{ASR} model, the alignments produced by the pipeline (denoted by $A_{pipeline}$) could now be compared to $A_{original}$. However, this would not be a valid comparison, because the \ac{VAD} stage might split the audio signal into different segments, resulting in different transcripts and consequently in different alignments. Instead of splitting the audio signal by the \ac{VAD} stage, the segmentation information from the corpus' meta data was used. $A_{pipeline}$ were then directly compared to the $A_{original}$ provided by the corpus meta data. Because $A_{original}$ is not just a hypothetical, but a perfect reference alignment, all differences to $P=R=F=1$ and $LER=0$ can be attributed to the \ac{ASR} and the \ac{GSA} stage in the pipeline.

%The \ac{VAD} stage in the German pipeline was therefore skipped for the German pipeline. Instead, the segmentation information from the corpus metadata was used. This way, the \ac{ASR} stage in the pipeline operates on exactly the same audio segments as are given from the corpus. The resulting partial transcripts (and therefore also the alignments produced by the \ac{GSA} stage) can therefore be compared directly with \textit{Alignments\_Original}. 

%Because the regularized model trained with synthesized data has both the lowest \ac{LER} rates and a smoother curve, this model is used in the \ac{ASR} stage of the pipeline when aligning German samples. The pipeline can then be evaluated like it was evaluated for English samples. However, while for English samples the Mozilla implementation of \textit{DeepSpeech} could be used as a reference model, such a reference model is not available for German\footnote{except proprietary \ac{STT} engines like \textit{Google Cloud Speech}, which are only available online} because the \textit{CommonVoice} German dataset is not mature enough. To still evaluate how well the pipeline does for German audio/transcript samples, the segmentation information available in the test data is used instead of splitting the audio signal into voiced parts. The speech segments are put through the other stages (\ac{ASR} and \ac{GSA}) like before. By doing so, the \ac{VAD} stage is canceled out and the results reflect the impact of the last two stages. The final alignments are then evaluated as before, i.e. by calculating $P$recision, $R$ecall and $F$-score, but without comparing the values to a reference model. Because there was no reference model, the inferred transcripts were not concatenated and compared to the original transcript.

The six audio/text samples from the test set of the \textit{ReadyLingua} corpus were used to evaluate the German pipeline. Because those samples all happen to be read by the same speaker, another six audio/text samples from the \textit{PodClub} corpus were used (being read by two additional speakers). This resulted in a test set of 12 entries with a total audio length of about 74 minutes. Figure \ref{pipeline_boxplot_rl_de} shows the boxplots for $P$, $R$ and $F$ for the alignments produced by the pipeline for those samples.

\begin{figure}[h!]
	\includegraphics[width=\linewidth]{./img/boxplot_rl.png}
	\caption{Average values of $P$, $R$ and $F$ for German audio/transcripts aligned with a pipeline using the simplified \ac{STT}. Precision is slightly higher than for the English pipeline (mean: 0.470, median: 0.453). However, this might be due to the fact that the German test set was much smaller (number of samples, total audio length) and also less diverse. The F-Score is similar to the one for the English pipeline (mean: 0.637, median: 0.626)}
	\label{pipeline_boxplot_rl_de}
\end{figure}

The results do not differ much from the ones achieved with the English pipeline. The mean values of $P$, $R$ and $F$ are actually even a big higher. However, we have to consider the fact that the simplified German \ac{ASR} model was trained on large amounts of synthesized data and is therefore more prone to overfitting, because much of the data was derived from only a few speakers. We also have to consider that the test-set used to evaluate the German pipeline was much smaller than the one used to evaluate the English pipeline, both in terms of the number of samples (12 instead of 84) as well as the total length of the samples (74 minutes instead of almost 22 hours). Because of the small size, this test set can not exhibit a realistic distribution of speakers, genders, accents, etc.. 

The average length of samples in the German test-set is also much shorter. Longer transcripts make a global alignment more challenging. Figure \ref{pipeline_scatterplot_rl_de} shows this very clearly: six of the twelve samples from the test set were taken from the \textit{PodClub} corpus and were longer than the other six taken from the \textit{ReadyLingua} corpus. Both groups form clusters in the left plot, visualizing the average \ac{LER} between transcript and alignment. The same clusters also appear in the right plot, visualizing the average \textit{Levenshtein Similarity} between transcript and alignment. The \ac{LER} rate was generally higher resp. the \textit{Levenshtein Similarity} smaller for the longer samples. Such a trend was not observed in the English pipeline. It is expected therefore that with the current setting the quality of alignments will deteriorate with increasing sample length for German samples.

\begin{figure}[h!]
	\includegraphics[width=\linewidth]{./img/scatterplot_rl.png}
	\caption{Average \ac{LER} (left plot) and \textit{Levenshtein Similiarity} (right plot) between transcript and alignment. Obviously shorter samples lead to better alignments because the \ac{LER} rate of the shorter transcripts from the \textit{ReadyLingua} corpus are much lower than the rates of the longer \textit{PodClub} samples. Vice versa the \textit{Levenshtein Similarity} decreases for longer samples.}
	\label{pipeline_scatterplot_rl_de}
\end{figure}

Because the split into train-, validation- and test-set was not done with the same diligence as in the \textit{LibriSpeech} corpus, it happens to consists only of female speakers with a Swiss accent. Above results show that alignment can be done for recordings with these properties. For a more general statement however, the pipeline would have to be evaluated on a bigger test set with a wider distribution.

\subsection{Summary}

This chapter gave an overview how a $5$-gram model for German similar to the one downloaded from Mozilla was trained on Wikipedia corpus and validated empirically. It also showed how a pipeline was built that aligns German audio/text samples. A \ac{STT} model was trained using augmented data from \textit{ReadyLingua}. Like with the English samples, the training progress was visualized with a learning curve. The best performance was achieved using an regularized model that has been trained on $1.000$ minutes of original and synthesized data. The pipeline performance was evaluated, yielding similar results like the English pipeline. However, it was evaluated on only 12 samples (74 minutes). This is not enough and does not reflect a realistic distribution of speaker properties. The pipeline for German needs to be evaluated further to make statements about its general capability.