{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring n-gram LM\n",
    "\n",
    "This Jupyter Notebook lets you explore some n-gram LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kenlm\n",
    "import random\n",
    "import langdetect\n",
    "from random import shuffle\n",
    "from util.lm_corpus_util import process_sentence\n",
    "from util.lm_util import load_lm, load_vocab, load_lm_and_vocab\n",
    "\n",
    "def create_test_pair(sentence):\n",
    "    words = sentence.lower().split()\n",
    "    sentence_original = ' '.join(words)\n",
    "    sentence_shuffled = sentence_original\n",
    "    while sentence_shuffled == sentence_original:\n",
    "        shuffle(words)\n",
    "        sentence_shuffled = ' '.join(words)\n",
    "    return sentence_original, sentence_shuffled\n",
    "\n",
    "def score_sentence(model, sentence):\n",
    "    score = model.score(sentence)\n",
    "    print(f'score for \\'{sentence}\\': ', score)\n",
    "    for prob, ngram_length, oov in model.full_scores(sentence):\n",
    "        print({'probability': prob, \"n-gram length\": ngram_length, \"oov?\": oov})\n",
    "    print(\"perplexity:\", model.perplexity(sentence))\n",
    "    print()\n",
    "    return score\n",
    "    \n",
    "def check_lm(model, sentences, language=None):\n",
    "    ok = True\n",
    "    for sentence in sentences:\n",
    "        language = language if language else {'en': 'english', 'de': 'german'}[langdetect.detect(sentence)]\n",
    "        print(f'original sentence ({language}):', sentence)\n",
    "        sentence = process_sentence(sentence, language=language)\n",
    "        print('normalized sentence:', sentence)\n",
    "        original, shuffled = create_test_pair(sentence)\n",
    "        print()\n",
    "        print('scoring original sentence: ')\n",
    "        score_original = score_sentence(model, original)\n",
    "        print('scoring shuffled sentence: ')\n",
    "        score_shuffled = score_sentence(model, shuffled)\n",
    "        if score_original < score_shuffled:\n",
    "            ok = False\n",
    "    if ok:\n",
    "        print('model seems to be OK')\n",
    "               \n",
    "english_sentences = [\n",
    "    'Language modeling is fun', # normal sentence\n",
    "    'New York', # only one shuffled variant (York New), which should have a lower probabilty\n",
    "    'adasfasf askjh aksf' # some OOV words\n",
    "]\n",
    "german_sentences = [\n",
    "    'Seine Pressebeauftragte ist ratlos.',\n",
    "    'Fünf Minuten später steht er im Eingang des Kulturcafés an der Zürcher Europaallee.',\n",
    "    'Den Leuten wird bewusst, dass das System des Neoliberalismus nicht länger tragfähig ist.',\n",
    "    'Doch daneben gibt es die beeindruckende Zahl von 30\\'000 Bienenarten, die man unter dem Begriff «Wildbienen» zusammenfasst.',\n",
    "    'Bereits 1964 plante die US-Airline Pan American touristische Weltraumflüge für das Jahr 2000.',\n",
    "]\n",
    "german_sayings = [\n",
    "    'Ich bin ein Berliner',\n",
    "    'Man soll den Tag nicht vor dem Abend loben',\n",
    "    'Was ich nicht weiss macht mich nicht heiss',\n",
    "    'Ein Unglück kommt selten allein',\n",
    "    'New York'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepSpeech (5-gram, 250k words)\n",
    "\n",
    "The following model was trained for the Mozilla implementation of DeepSpeech and is included in [download of the pre-trained model](https://github.com/mozilla/DeepSpeech#getting-the-pre-trained-model). The model's vocabulary is contained in the file  (). The file `vocab.txt` contatins the vocabulary of the model (one word per line), which comprises also very exotic words and probably spelling errors and is therefore very big (973.673 words). To train the \\ac{LM}, $n$-grams of order 4 and 5 were pruned with a threshold value of 1, meaning only 4- and 5-grams with a minimum count of 2 and higher are estimated ([see the details about how Mozilla trained the LM](https://github.com/mozilla/DeepSpeech/tree/master/data/lm)). Because spelling errors are probably unique within the training corpus, 4- or 5-grams containing a misspelled word are unique too and are therefore pruned. \n",
    "\n",
    "Such a large vocabulary is counter-productive to use in a spell checker because it raises the probability that minor misspellings are \"corrected\" to the wrong word or that a very rare or misspelled word is used. Unfortunately,`vocab.txt` does not contain any information about how often it appears in the corpus. Therefore, a vocabulary of the 250.000 most frequent word in standard format (one line, words separated by single space) is created using the following commands:\n",
    "\n",
    "```bash\n",
    "n=250000 # use 250k most frequent words\n",
    "\n",
    "# download file\n",
    "wget http://www.openslr.org/resources/11/librispeech-lm-norm.txt.gz\n",
    "\n",
    "# decompress file\n",
    "gunzip librispeech-lm-norm.txt.gz\n",
    "\n",
    "# count word occurrences and keep n most frequent words\n",
    "cat librispeech-lm-norm.txt |\n",
    "    pv -s $(stat --printf=\"%s\" librispeech-lm-norm.txt) | # show a progress bar\n",
    "    tr '[:upper:]' '[:lower:]' | # lowercase everything\n",
    "    tr -s '[:space:]' '\\n' | # replace spaces with one newline\n",
    "    sort | # sort alphabetically\n",
    "    uniq -c | # count occurrences\n",
    "    sort -bnr | # numeric sort\n",
    "    tr -d '[:digit:] ' | # remove counts from lines\n",
    "    head -${n} | # keep n most frequent words words\n",
    "    tr '\\n' ' ' > lm.vocab # replace line breaks with spaces and write to lm.vocab\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_lm('/media/daniel/IP9/lm/ds_en/lm.binary')\n",
    "check_lm(model, english_sentences, 'english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom model (4-gram, details unknown)\n",
    "\n",
    "The following model was trained on the TIMIT corpus and downloaded from https://www.dropbox.com/s/2n897gu5p3o2391/libri-timit-lm.klm. Details as the vocabulary or the data structure are not known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_lm('/media/daniel/IP9/lm/timit_en/libri-timit-lm.klm')\n",
    "check_lm(model, english_sentences, 'english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LibriSpeech (4-gram)\n",
    "\n",
    "The following model has been trained on the LibriSpeech corpus. The ARPA file was downloaded from http://www.openslr.org/11. The ARPA model has been lowercased for the sake of consistence. Apart from that, no other preprocessing was done. The model was trained using a vocabulary of 200k words.\n",
    "\n",
    "A KenLM binary model was trained on the lowercased ARPA model using the _Trie_ data structure. This data structure is also what was used to train the German model (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_lm('/media/daniel/IP9/lm/libri_en/librispeech-4-gram.klm')\n",
    "check_lm(model, english_sentences, 'english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## German models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SRI model (3-Gram, CMUSphinx)\n",
    "The following is a 3-gram LM that has been trained with CMUSphinx. The ARPA file was downloaded from https://cmusphinx.github.io/wiki/download/ and converted to a binary KenLM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_lm('/media/daniel/IP9/lm/srilm_de/srilm-voxforge-de-r20171217.klm')\n",
    "check_lm(model, german_sentences, 'german')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom KenLM (2-gram, probing, all words)\n",
    "\n",
    "The following 2-gram model was trained on sentences from articles and pages in a Wikipedia dump. The dump was downloaded on 2018-09-21 and contains the state from 2018-09-01. The current dump of the German Wikipedia can be downloaded at http://download.wikimedia.org/dewiki/latest/dewiki-latest-pages-articles.xml.bz2.\n",
    "\n",
    "The model was not pruned. Probing was used as data structure. The following command was used to create the model:\n",
    "\n",
    "```bash\n",
    "lmplz -o 2 -T /home/daniel/tmp -S 40% <wiki_de.txt.bz2 | build_binary /dev/stdin wiki_de_2_gram.klm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_lm('/media/daniel/IP9/lm/wiki_de/wiki_de_2_gram.klm')\n",
    "check_lm(model, german_sentences, 'german')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom KenLM (4-gram, trie, 500k words)\n",
    "\n",
    "The following 4-gram model was trained on the same dump like the 2-gram model above, but with a limited vocabulary of the first 500k most frequent words in the corpus. Additionally, a _Trie_ was used as data structure instead of the hash table in _Probing_. The model was built with the following program\n",
    "\n",
    "```bash\n",
    "lmplz --order 4 \\\n",
    "      --temp_prefix /tmp/ \\\n",
    "      --memory 40% \\\n",
    "      --limit_vocab_file wiki_de_500k.vocab \\\n",
    "      --text wiki_de.txt.bz2 \\\n",
    "      --arpa wiki_de_trie_4_gram_500k.arpa\n",
    "      \n",
    "build_binary trie wiki_de_trie_4_gram_500k.arpa wiki_de_trie_4_gram_500k.klm\n",
    "```\n",
    "\n",
    "Where `wiki_de.txt.bz2` is the training corpus and `wiki_de_500k.vocab` is a text file containing the 500k most frequent words from the training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_lm('/media/daniel/IP9/lm/wiki_de/wiki_de_4_gram_500k_trie.klm')\n",
    "check_lm(model, german_sentences, 'german')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom KenLM (5-gram, trie, pruned)\n",
    "\n",
    "The following model was trined like the 4-gram model above, but with a higher order (5-gram instead of 4-gram). Additionally, the vocabulary was not pruned. The model was quantized with 8 bits and pointers were compressed to save memory.\n",
    "\n",
    "```bash\n",
    "lmplz --order 5 \\\n",
    "      --temp_prefix /tmp/ \\\n",
    "      --memory 40% \\\n",
    "      --text wiki_de.txt.bz2 \\\n",
    "      --arpa wiki_de_5_gram_pruned.arpa\n",
    "      \n",
    "build_binary -a 255 \\\n",
    "             -q 8 \\\n",
    "             trie wiki_de_5_gram_pruned.arpa \\\n",
    "             wiki_de_5_gram_pruned.klm\n",
    "```\n",
    "\n",
    "The file `wiki_de_5_gram_pruned.klm` is the binary KenLM model that was used to implement a simple spell checker in this project. The spell checker uses a truncated vocabulary of the 250k most frequent words and the model is then used to calculate the likelihood (score) for each sentence. Note that although the spell checker uses a truncated vocabulary, the model was trained on the full text corpus without limiting the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_lm('/media/daniel/IP9/lm/wiki_de/wiki_de_5_gram_pruned.klm')\n",
    "check_lm(model, german_sentences, 'german')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple word predictor\n",
    "\n",
    "The trained model can be used together with its vocabulary to create a simple word predictor that lets you start a sentence and will propose possible continuations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def predict_next_word(model, vocab, language):\n",
    "    inp = input('Your turn now! Enter a word or the beginning of a sentence and the LM will predict a continuation. Enter nothing to quit.\\n')\n",
    "    sentence = process_sentence(inp, language)\n",
    "    while (inp):\n",
    "        score = model.score(sentence, bos=False, eos=False)\n",
    "        print(f'score for \\'{sentence}\\': {score}')        \n",
    "        top_5 = sorted(((word, model.score(sentence.lower() + ' ' + word)) for word in vocab), key=lambda t: t[1], reverse=True)[:5]\n",
    "        print(f'top 5 words:')\n",
    "        print(tabulate(top_5, headers=['word', 'log10-probability']))\n",
    "        inp = input('Enter continuation:\\n')\n",
    "        sentence += ' ' + process_sentence(inp, language)\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.lm_util import load_lm, load_vocab\n",
    "from util.lm_corpus_util import process_sentence\n",
    "\n",
    "model = load_lm('/media/daniel/IP9/lm/ds_en/lm.binary')\n",
    "vocab = load_vocab('/media/daniel/IP9/lm/ds_en/lm_80k.vocab')\n",
    "\n",
    "predict_next_word(model, vocab, 'german')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from util.lm_util import load_lm, load_vocab\n",
    "from util.lm_corpus_util import process_sentence\n",
    "\n",
    "model = load_lm('/media/daniel/IP9/lm/wiki_de/wiki_de_5_gram.klm')\n",
    "vocab = load_vocab('/media/daniel/IP9/lm/wiki_de/wiki_de_80k.vocab')\n",
    "\n",
    "predict_next_word(model, vocab, 'german')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple spell checker\n",
    "\n",
    "The trained model together with its vocabulary can be used to implement a simple spell checker. For each word of a sentence, the spell checker checks if it appears in the vocabulary. If it does, it is not changed. If it does not, all words in the vocabulary with edit distance 1 are searched. If there are none, all words in the vocabulary with edit distance 2 are searched. If there are none, the original word is kept. This is done for each word in the sentence. The spell checker then calculates the probabilities for all combinations of words using beam search with a beam width of 1024. The most probable combination is used as corrected sentence. The following sections illustrate examples for English and German."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.lm_util import load_lm, load_vocab, correction\n",
    "\n",
    "model = load_lm('/media/daniel/IP9/lm/ds_en/lm.binary')\n",
    "vocab = load_vocab('/media/daniel/IP9/lm/ds_en/lm_80k.vocab')\n",
    "\n",
    "sentence = 'i seee i sey saind the blnd manp to his deaf dauhgter'\n",
    "sentence_corr = correction(sentence, language='en', lm=model, lm_vocab=vocab)\n",
    "\n",
    "print(f'original sentence:  {sentence}')\n",
    "print(f'corrected sentence: {sentence_corr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.lm_util import load_lm, load_vocab, correction\n",
    "\n",
    "model = load_lm('/media/daniel/IP9/lm/wiki_de/wiki_de_5_gram.klm')\n",
    "vocab = load_vocab('/media/daniel/IP9/lm/wiki_de/wiki_de_80k.vocab')\n",
    "print('superheld' in vocab)\n",
    "\n",
    "sentence = 'man isd nur dannn ein supeerheld wenn man sihc selbsd fur supehr häält'\n",
    "sentence_corr = correction(sentence, language='de', lm=model, lm_vocab=vocab)\n",
    "\n",
    "print(f'original sentence:  {sentence}')\n",
    "print(f'corrected sentence: {sentence_corr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "279px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
