\section{Creating a Language Model for German}\label{lm}

The learning curve above was plotted using the results from training on English samples. To compare get an intuition about whether the conclusions made are transferable to training on samples in other languages, the training was done again on German samples. In order to do this, a language model for German had to be created first. A 4-gram LM was trained on a corpus of 2.844.448 articles and pages from Wikipedia (104.327.682 sentences, xxx words, yyy unique words) using KenLM \cite{kenlm}. The corpus was normalized by removing Wiki markup, any punctuation and making everything lowercase. Accentuated characters like \code{è,é,ê, ...} and special characters like the German \textit{ß} were translated to their most similar ASCII-equivalent (\code{e} resp. \code{ss}) to account for possible spelling errors and reduce the number of words. Umlauts (although not part of the ASCII codeset) were kept as-is however. $n$-grams can be represented with a tree structure \footnote{note that \textit{KenLM} offers a s.c. \textit{PROBING} data structure, which is fundamentally a hash table combined with interpolation search, a more sophisticated variant of binary search, which allows for constant space complexity and linear time complexity. This does however not change the fact that $n$-grams can conceptually be thought as a tree of grams}, which allows for pruning. The $n$-grams used to train the \ac{LM} have been pruned by setting the minimal threshold for any $n$-Gram ($n \in 1..4$) to 100. Pruning the unigrams was done to get rid of obvious spelling mistakes. Pruning higher-order $n$-grams was done to increase performance (both in space and time).