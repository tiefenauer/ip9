{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these parameters to load a different model\n",
    "model = \"/home/daniel/models/output_graph.pb\"\n",
    "alphabet = \"/home/daniel/models/alphabet.txt\"\n",
    "lm = \"/home/daniel/models/lm.binary\"\n",
    "trie = \"/home/daniel/models/trie\"\n",
    "\n",
    "# These constants control the beam search decoder\n",
    "BEAM_WIDTH = 500  # Beam width used in the CTC decoder when building candidate transcriptions\n",
    "LM_WEIGHT = 1.75  # The alpha hyperparameter of the CTC decoder. Language Model weight\n",
    "# Valid word insertion weight. This is used to lessen the word insertion penalty\n",
    "# when the inserted word is part of the vocabulary\n",
    "VALID_WORD_COUNT_WEIGHT = 1.00\n",
    "# These constants are tied to the shape of the graph used (changing them changes\n",
    "# the geometry of the first layer), so make sure you use the same constants that\n",
    "# were used during training\n",
    "# Number of MFCC features to use\n",
    "N_FEATURES = 26\n",
    "# Size of the context window used for producing timesteps in the input vector\n",
    "N_CONTEXT = 9\n",
    "\n",
    "from deepspeech.model import Model\n",
    "import sys\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# load pre-trained DeepSpeech model from file\n",
    "print(f'Loading model from file {model}', file=sys.stderr)\n",
    "model_load_start = timer()\n",
    "ds = Model(model, N_FEATURES, N_CONTEXT, alphabet, BEAM_WIDTH)\n",
    "model_load_end = timer() - model_load_start\n",
    "print(f'Loaded model in {model_load_end:.3}s.', file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corpus\n",
    "from util.corpus_util import *\n",
    "\n",
    "# corpus = get_corpus('rl')(languages='en')\n",
    "corpus = get_corpus('ls')\n",
    "corpus.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, Audio, display\n",
    "from pattern3.metrics import levenshtein_similarity\n",
    "import random\n",
    "\n",
    "def get_random_test_samples(corpus, num=5):\n",
    "    print(f'selecting {num} random speech segments from testset (corpus: {corpus.name})')\n",
    "    test_entries = [entry for entry in corpus if entry.subset.startswith('test')]\n",
    "    return [random.choice(entry.speech_segments) for entry in random.sample(test_entries, num)]\n",
    "\n",
    "segments = get_random_test_samples(corpus)\n",
    "\n",
    "for i, segment in enumerate(segments):\n",
    "    print(f'Inferring transcription for speech segment #{i}')\n",
    "    audio, rate = segment.audio, segment.rate\n",
    "    transcription = ds.stt(audio, rate)\n",
    "    \n",
    "    display(HTML(f'<strong>From corpus entry</strong>: {segment.corpus_entry.name} ({segment.corpus_entry.id})'))    \n",
    "    display(Audio(data=audio, rate=rate))\n",
    "    display(HTML(f'<strong>inferred transcription</strong>:<br/>{transcription}'))    \n",
    "    display(HTML(f'<strong>actual transcription</strong>:<br/>{segment.text}'))\n",
    "    display(HTML(f'<strong>Levenshtein similarity (=LER)</strong>: {levenshtein_similarity(transcription, segment.text)}'))\n",
    "    display(HTML(f'<hr/>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
