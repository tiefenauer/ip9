\section{Introduction}\label{intro}
This report documents the progress of the project \textit{Forced Alignment with a Recurrent Neural Network}. The project serves as a master thesis at FHNW (IP9). Some preliminary work has been done in a previous project (IP8). The overall goal, project situation and background information are described in detail in the project report for IP8 and are not repeated here. Instead, a quick recap is given for the sake of completeness of this documentation.

\subsection{Scope and overall goal}
\textit{ReadyLingua} is a Switzerland based company that develops tools and produces content for language learning. Some of this content consists of audio/video data with an accompanying transcript. The overall goal is to enrich this data with temporal information, so that for each part of the transcript the corresponding point in the audio/video data can be found. This process is called \textit{Forced Alignment}. An \textit{InnoSuisse} project was started in 2018 to research how this could be achieved. The \textit{InnoSuisse} project foresees three different approaches, one of which is followed in this project.

\subsection{Chosen approach and previous work}
The approach chosen in this project is based on speech pauses, which can be detected using \textit{Voice Activity Detection} (VAD). The utterances in between are transcribed using \textit{Automatic Speech Recognition} (ASR), for which a \textit{Recurrent Neural Network} (RNN) is used. The resulting partial transcripts contain the desired temporal information and can be matched up with the full transcript with a process called \textit{Local Sequence Alignment} (LSA)

In the IP8 project, VAD, ASR and LSA were treated as part of a pipeline which split a given audio file into individual utterances, transcribe them and localize them in the original transcript. Since the quality of the ASR stage has an imminent impact on the downstream LSA stage, the quality of the alignments is heavily dependent on the quality of the partial transcripts. However, ASR is highly prone to external influences like background noise, properties of the speaker (gender, speaking rate, pitch, loudness). Apart from that, language is inherently abiguous (e.g. accents), inconsistent (e.g. linguistic subtleties like homonyms or homophones) and messy (stuttering, unwanted repetitions, mispronunciation).

\subsubsection{Previous results}
For the VAD stage, an implementation of WebRTC was used which was shown to be capable of detecting utterances with very high accuracy within reasonable time. For the LSA stage a combination of the Smith-Waterman algorithm and the Levenshtein distance was used. This combination included tunable parameters and proved to be able to be able to localize partial transcript within the full transcript pretty well, provided the similarity between actual and predicted text was high enough.

Because the final pipeline should be language-agnostic, the IP8 project proposed the use of \textit{DeepSpeech} for the ASR stage, which uses CTC\cite{ctc_paper} as its cost function. It included some experiments on what features could be used to train a RNN for the ASR stage. Possible features were raw power-spectrograms (as stipulated by the \textit{DeepSpeech} paper), Mel-Spectrograms and MFCC. It was found that training on MFCC features would probably require the least amount of training data because. An RNN using a simplified version of the \textit{DeepSpeech} architecture was trained on data from the \textit{LibriSpeech} project (containing only English samples). However, developing a fully-fletched ASR system is extremely time-consuming and could not be done within the project time. For that reason a state-of-the-art \textit{Speech-To-Text} (STT) engine (\textit{Google Cloud Speech}) was embedded in the pipeline as the ASR stage. Using this engine, the pipeline was able to produce very good (although not perfect) transcripts for the individual utterances. Therefore the chosen approach was validated and the pipeline could shown to be generally functional.

\subsection{Goal of this project}

In this project, the chosen pipelined approach shall further be refined. Because the VAD and the LSA stage already work pretty well, the focus in this project is on the ASR stage. By researching if and how an own STT engine can be obtained the pipeline shall become self-contained by using an ASR stage that is not dependent on a proprietary solution and can be individually tuned. Moreover the quality of the produced alignment shall be assessed. Since this is the overall goal, it shall become clear how this quality changes with varying properties of the ASR system.

Concretely, the following aspects shall be examined more closely:

\begin{itemize}
	\item \textbf{How does the quality of the simplified \textit{DeepSpeech}-RNN change with increasing training data?} By plotting the learning curve we should be able to see whether the RNN is able to learn something useful at all and also get some intuition about how much training data is needed to get reasonably accurate partial transcripts.
	\item \textbf{How does the quality of the partial transcripts change when using synthesized training data?} Neural Network usually require large amounts of training data and often improve with increasing size of the training set. However, labelled training data is usually scarce and expensive to acquire. For the purpose of Forced Alignment however, synthesized training data can be easily obtained by adding some distortion to the original signal (reverb, change of pitch, change of tempo).
	\item \textbf{How does the quality of the partial transcript change when integrating a language model (LM)?} STT-engines traditionally use a LM that models the probabilities of characters, words or sentences. A LM can help producing valid transcripts by mapping transcripts (that may sound similar to what was actually said) to orthographically correct sentences.
	\item \textbf{How can we assess the quality of the alignments?} This should give us some insight about how the quality of the alignment changes with varying capability of the STT-engine and what quality of transcripts is required.
\end{itemize}

The answers to above questions should help in estimating the effort to create a generic solution (amount of training data, architecture, etc.). \footnote{Because ASR is highly dependent on the language that should be recognized, a different STT system has to be trained for each language.}