\section{Introduction}\label{intro}
This report documents the progress of the project \textit{Speech-To-Text Engine for Forced Alignment}, my master thesis at \ac{FHNW} (referred to as \textit{IP9}). Some preliminary work has been done in a previous project (referred to as \textit{IP8}). The overall goal, project situation and some background information are described in detail in the project report for IP8 and shall not be repeated here. Only a quick recap of the relevant terms and aspects is given as far as they are relevant for the understanding of this document.

\subsection{Scope and overall goal}
\textit{ReadyLingua} is a Switzerland based company that develops tools and produces content for language learning. Some of this content consists of audio/video data with an accompanying transcript. The overall goal is to enrich the textual data with temporal information, so that for each part of the transcript the corresponding point in the audio/video data can be found. This process is called \textit{\ac{FA}}. An \textit{InnoSuisse} project was started in 2018 to research how this could be achieved. The \textit{InnoSuisse} project foresees three different approaches, one of which is pursued in this project.

\subsection{Chosen approach and previous work}
The approach chosen for this project is based on speech pauses, which can be detected using \textit{\ac{VAD}}. The utterances in between are transcribed using \textit{\ac{ASR}}, for which a \textit{\ac{RNN}} is used. The resulting partial transcripts contain the desired temporal information and can be matched up with the full transcript by means of \ac{LSA}.

All theses parts were treated as stages of a pipeline:

\begin{itemize}
	\item \textbf{\ac{VAD}}: the audio was split into non-silent parts
	\item \textbf{\ac{ASR}}: each part was transcribed resulting in a partial transcript, which can contain transcription errors
	\item \textbf{\ac{LSA}}: each partial transcript was localized within the original transcript	
\end{itemize}

Since the quality of the \ac{ASR} stage has an imminent impact on the subsequent \ac{LSA} stage, the quality of the alignments depends heavily on the quality of the partial transcripts. This makes the \ac{ASR} stage the crucial stage of the pipeline. However, \ac{ASR} is highly prone to external influences like background noise, properties of the speaker (gender, speaking rate, pitch, loudness). Apart from that, language is inherently abiguous (e.g. accents), inconsistent (e.g. linguistic subtleties like homonyms or homophones) and messy (stuttering, unwanted repetitions, mispronunciation).

\subsubsection{Previous results and problems}
For the \ac{VAD} stage, an implementation\footnote{\url{https://github.com/wiseman/py-webrtcvad}} of \textit{WebRTC}\footnote{\url{https://webrtc.org/}} was used. This implementation which has proved to be capable of detecting utterances with very high accuracy within reasonable time. For the \ac{LSA} stage a combination of the Smith-Waterman algorithm and the Levenshtein distance was used. This combination included tunable parameters and proved to be able to be able to localize partial transcript within the full transcript pretty well, provided the similarity between actual and predicted text was high enough. 

For the \ac{ASR} stage on the other hand, no \ac{RNN} could be trained that was capable of transcribing the audio segments with a quality that was high enough for the \ac{LSA} stage. The main problems were the lack of readily available training data in high quality, very long training times and therefore very long feedback cycles. Because the \ac{ASR} stage is at the heart of the pipeline, the self-trained model was replaced by \ac{GCS}\footnote{\url{https://cloud.google.com/speech-to-text/}}, which was embedded into the pipeline over an API. This step however made the pipeline dependent on a commercial product, whose inner workings remain unknown and who cannot be tuned to the project's needs. Furthermore, although the quality of the transcriptions produced by \ac{GCS} is very good, it might be an overkill for the purpose of this project and the API calls are subject to charges.

The IP8 project proposed the use of \textit{DeepSpeech} for the \ac{ASR} stage, which uses \ac{CTC} \parencite{ctc_paper} as its cost function. Some experiments were made to find out what features can be used to train a \ac{RNN} for the \ac{ASR} stage. The features considered were raw power-spectrograms (as stipulated by the \textit{DeepSpeech} paper), Mel-Spectrograms and \ac{MFCC}. It was found that training on \ac{MFCC} features would probably require the least amount of training data because. An \ac{RNN} using a simplified version of the \textit{DeepSpeech} architecture was trained on data from the \textit{LibriSpeech} project (containing only English samples). However, developing a fully-fletched ASR system is extremely time-consuming and could not be done within the project time. For that reason a state-of-the-art \textit{\ac{STT}} engine (\textit{Google Cloud Speech}) was embedded in the pipeline as the \ac{ASR} stage. Using this engine, the pipeline was able to produce very good (although not perfect) transcripts for the individual utterances. Therefore the chosen approach was validated and the pipeline could shown to be generally functional.

\subsection{Goal of this project}

In this project, the chosen pipelined approach shall further be refined. Because the \ac{VAD} and the \ac{LSA} stage already work pretty well, the focus in this project lies on the \ac{ASR} stage. Because the pipeline should become language-agnostic and self-contained, a \ac{RNN} must be trained that can be used in this stage in the pipeline. Such a \ac{RNN} could be a simplified variant of the \textit{DeepSpeech} model. The idea behind this are the following two points:

\begin{itemize}
	\item A simpler model will not be able to produce transcripts with the same quality as complex models like \textit{DeepSpeech} or \ac{GCS}. But since the superordinated goal of this project is \ac{FA} and not Speech Recognition, this may not be required in the first place. The \ac{RNN} only needs to be \textit{good enough} for the downstream \ac{LSA} stage
	\item On the other hand, a simpler model will probably need less training data to learn. This might open up new possibilities to align audio and text in other languages, for which there is not a third-party solution yet, such as Swiss German.
\end{itemize}

The goal of this project is therefore to make statements as to under what conditions the \ac{ASR} stage can be implemented. For this, various combinations of network or data properties are explored as well as varying amounts of training data. Concretely, the following questions shall be answered:

\begin{itemize}
	\item \textbf{How does the quality of the simplified \textit{DeepSpeech}-\ac{RNN} change with increasing training data?} By plotting the learning curve we should be able to see whether the RNN is able to learn something useful at all and also get some intuition about how much training data is needed to get reasonably accurate partial transcripts.
	\item \textbf{How does the quality of the partial transcripts change when using synthesized training data?} Neural Network usually require large amounts of training data and often improve with increasing size of the training set. However, labelled training data is usually scarce and/or expensive to acquire. For the purpose of Forced Alignment however, synthesized training data can be easily obtained by adding some distortion to the original signal (reverb, change of pitch, change of tempo).
	\item \textbf{How does the quality of the partial transcript change when integrating a \ac{LM}?} \ac{STT}-engines traditionally use a \ac{LM} that models the probabilities of characters, words or sentences. A \ac{LM} can help producing valid transcripts by mapping transcripts (that may sound similar to what was actually said) to orthographically correct sentences.
	\item \textbf{How can we assess the quality of the alignments?} This should give us some insight about how the quality of the alignment changes with varying capability of the \ac{STT}-engine and what quality of transcripts is required.
\end{itemize}

Answering above questions should help estimating the effort to create a generic pipeline. \footnote{Because \ac{ASR} is highly dependent on the language that should be recognized, a different \ac{STT} system has to be trained for each language.}

