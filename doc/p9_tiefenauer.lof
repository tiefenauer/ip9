\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax 
\babel@toc {english}{}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1}{\ignorespaces Architecture of the simplified model. The cell type and the activation function is indicated in brackets for each layer (FC=Fully-Connected, ReLU=Rectified Linear Unit)\relax }}{7}{figure.caption.25}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2}{\ignorespaces Example of how the spell checker works\relax }}{10}{figure.caption.26}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3}{\ignorespaces Learning curve for the CTC-loss while training on 1/10/100/1000 minutes of transcribed audio from the \ac {CV} corpus using the $5$-gram \ac {LM} provided by the Mozilla implementation of \textit {DeepSpeech}\relax }}{15}{figure.caption.32}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4}{\ignorespaces Learning curve for the \ac {LER} metric while training on 1/10/100/1000 minutes of transcribed audio from the \ac {CV} corpus with and without spelling correction with a \ac {LM}. For the lines where spelling was corrected, the $5$-gram \ac {LM} provided by the Mozilla implementation of \textit {DeepSpeech} was used.\relax }}{16}{figure.caption.33}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5}{\ignorespaces Learning curve for the \ac {WER} metric while training on 1/10/100/1000 minutes of transcribed audio from the \ac {CV} corpus with and without spelling correction with a \ac {LM}. For the lines where spelling was corrected, the $5$-gram \ac {LM} provided by the Mozilla implementation of \textit {DeepSpeech} was used.\relax }}{16}{figure.caption.34}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6}{\ignorespaces Average values of $P$, $R$ and $F$ for a pipeline using the simplified \ac {STT} model compared to a pipeline using a state of the art model\relax }}{21}{figure.caption.39}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {7}{\ignorespaces average \ac {LER} between transcript and alignment (left plot) and similarity between alignments made by using the pre-trained \textit {DeepSpeech} model and the simplified model (right plot). The average \ac {LER} values when using the simplified model (mean: $0.982$, median: $0.800$) follow the same pattern like when using the reference model (mean: $0.230$, median: $0.148$), but are generally higher due to the lower quality of the transcriptions. However, this does not seem to affect the alignments produced by the two pipelines. The average similarity between alignments produced by the pipeline using the simplified model are roughly the same like when using the reference model (mean: $0.887$, median: $0.909$).\relax }}{22}{figure.caption.40}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {8}{\ignorespaces Word predictions of the trained 5-gram model for continuations of the stump \foreignquote *{french}{\textit {Ein 2007 erschienenes ...}}. The blue path represents a grammatically valid German sentence.\relax }}{28}{figure.caption.46}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {9}{\ignorespaces Impact of regularization and/or synthetisation on the progress of average \ac {LER} values. Regularization alone (left) will lead to lower \ac {LER} rates. Synthesized training data (middle) will lead to a smoother curve and lower \ac {LER} rates. Both measures combined will also combine their effects, although the trend is less obvious.\relax }}{29}{figure.caption.47}
